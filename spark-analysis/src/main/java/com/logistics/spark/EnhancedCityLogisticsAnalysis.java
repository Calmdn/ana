package com.logistics.spark;

import com.logistics.spark.monitor.SparkJobMonitor;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.expressions.Window;
import org.apache.spark.sql.functions;
import org.apache.spark.sql.types.DataTypes;
import static org.apache.spark.sql.functions.*;
import scala.collection.JavaConverters;
import java.util.Arrays;

import java.util.Properties;

/**
 * ç‰©æµæ•°æ®åˆ†æç³»ç»Ÿ
 *
 * åŠŸèƒ½æ¨¡å—ï¼š
 * 1. æ—¶é—´æ•ˆç‡åˆ†æ - é…é€æ—¶æ•ˆã€å–ä»¶æ—¶æ•ˆåˆ†æ
 * 2. ç©ºé—´åœ°ç†åˆ†æ - çƒ­åŠ›å›¾ã€é…é€å¯†åº¦ã€åœ°ç†è¦†ç›–
 * 3. è¿è¥æ•ˆç‡åˆ†æ - å¿«é€’å‘˜æ•ˆç‡ã€åŒºåŸŸè´Ÿè½½åˆ†æ
 * 4. é¢„æµ‹åˆ†ææ•°æ® - è¶‹åŠ¿é¢„æµ‹ã€å®¹é‡è§„åˆ’æ•°æ®
 * 5. æˆæœ¬æ•ˆç›Šåˆ†æ - æˆæœ¬ç»“æ„ã€æ•ˆç›Šè¯„ä¼°
 * 6. KPIç›‘æ§æŒ‡æ ‡ - æ ¸å¿ƒæŒ‡æ ‡ã€æœåŠ¡è´¨é‡ç›‘æ§
 * 7. å¼‚å¸¸æ£€æµ‹åˆ†æ - æ—¶é—´å¼‚å¸¸ã€è·ç¦»å¼‚å¸¸æ£€æµ‹
 * 8. ç»¼åˆæŠ¥è¡¨æ•°æ® - æ—¥æŠ¥ã€å‘¨æŠ¥ã€æœˆæŠ¥æ•°æ®
 *
 * åŒæ—¶æ”¯æŒHDFSå­˜å‚¨å’ŒMySQLå®æ—¶æ•°æ®å†™å…¥
 */
public class EnhancedCityLogisticsAnalysis {

    // MySQLè¿æ¥é…ç½®
    private static Properties mysqlProps;
    private static String mysqlUrl;
    private static SparkJobMonitor jobMonitor; // æ·»åŠ ç›‘æ§å™¨

    public static void main(String[] args) {
        if (args.length != 3) {
            System.err.println("ä½¿ç”¨æ–¹æ³•: EnhancedCityLogisticsAnalysis <deliver_path> <pickup_path> <output_path>");
            System.err.println("ç¤ºä¾‹: spark-submit --class com.logistics.spark.EnhancedCityLogisticsAnalysis " +
                    "app.jar hdfs://localhost:9000/data/deliver/* hdfs://localhost:9000/data/pickup/* " +
                    "hdfs://localhost:9000/output/analysis");
            System.exit(1);
        }

        String deliverPath = args[0];
        String pickupPath = args[1];
        String outputPath = args[2];

        // åˆå§‹åŒ–MySQLè¿æ¥é…ç½®
        initMySQLConfig();

        // åˆå§‹åŒ–ç›‘æ§å™¨
        jobMonitor = new SparkJobMonitor(mysqlProps, mysqlUrl);

        // åˆå§‹åŒ–Sparkä¼šè¯
        SparkSession spark = SparkSession.builder()
                .appName("ç‰©æµåˆ†æç³»ç»Ÿ")
                .master("local[*]")
                .config("spark.sql.adaptive.enabled", "true")
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
                .config("spark.sql.adaptive.advisoryPartitionSizeInBytes", "128MB")
                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
                .getOrCreate();

        Long jobId = null;
        long totalProcessedRecords = 0;
        String errorMessage = null;
        boolean success = false;

        try {
            System.out.println("=== å¯åŠ¨å¢å¼ºç‰ˆç‰©æµæ•°æ®åˆ†æ ===");
            System.out.println("é…é€æ•°æ®è·¯å¾„: " + deliverPath);
            System.out.println("å–ä»¶æ•°æ®è·¯å¾„: " + pickupPath);
            System.out.println("è¾“å‡ºè·¯å¾„: " + outputPath);

            // ğŸš€ å¼€å§‹ä½œä¸šç›‘æ§
            jobId = jobMonitor.startJobTracking(
                    "ç‰©æµåˆ†æç³»ç»Ÿ",
                    deliverPath,
                    pickupPath,
                    outputPath,
                    "yyyy-MM-dd HH:mm:ss",  // æ—¶é—´æ ¼å¼
                    2024  // é»˜è®¤å¹´ä»½
            );

            // åŠ è½½å’Œæ¸…æ´—æ•°æ®
            Dataset<Row> deliverRaw = spark.read()
                    .option("header", "true")
                    .option("inferSchema", "true")
                    .csv(deliverPath);

            Dataset<Row> pickupRaw = spark.read()
                    .option("header", "true")
                    .option("inferSchema", "true")
                    .csv(pickupPath);

            long rawDeliverCount = deliverRaw.count();
            long rawPickupCount = pickupRaw.count();
            System.out.println("åŸå§‹æ•°æ®è®°å½•æ•° - é…é€: " + rawDeliverCount + ", å–ä»¶: " + rawPickupCount);

            // ğŸ“Š æ›´æ–°åˆå§‹è¿›åº¦
            jobMonitor.updateJobProgress(jobId, rawDeliverCount + rawPickupCount, "æ•°æ®åŠ è½½å®Œæˆ");

            // æ•°æ®æ¸…æ´—å’Œè½¬æ¢
            Dataset<Row> deliverClean = cleanAndTransformDeliveryData(deliverRaw);
            Dataset<Row> pickupClean = cleanAndTransformPickupData(pickupRaw);

            long cleanDeliverCount = deliverClean.count();
            long cleanPickupCount = pickupClean.count();
            totalProcessedRecords = cleanDeliverCount + cleanPickupCount;

            System.out.println("æ¸…æ´—åæ•°æ®è®°å½•æ•° - é…é€: " + cleanDeliverCount + ", å–ä»¶: " + cleanPickupCount);

            // ğŸ“Š æ›´æ–°æ¸…æ´—åè¿›åº¦
            jobMonitor.updateJobProgress(jobId, totalProcessedRecords, "æ•°æ®æ¸…æ´—å®Œæˆ");

            // ç¼“å­˜æ¸…æ´—åçš„æ•°æ®
            deliverClean.cache();
            pickupClean.cache();

            // æ‰§è¡Œ8ä¸ªåˆ†ææ¨¡å—
            System.out.println("\n=== å¼€å§‹æ‰§è¡Œåˆ†ææ¨¡å— ===");

            // 1. æ—¶é—´æ•ˆç‡åˆ†æ
            System.out.println("ğŸ“Š æ‰§è¡Œæ—¶é—´æ•ˆç‡åˆ†æ...");
            generateTimeEfficiencyMetrics(deliverClean, pickupClean, outputPath + "/time_efficiency", spark);
            jobMonitor.logModuleCompletion(jobId, "æ—¶é—´æ•ˆç‡åˆ†æ", totalProcessedRecords);

            // 2. ç©ºé—´åœ°ç†åˆ†æ
            System.out.println("ğŸ—ºï¸ æ‰§è¡Œç©ºé—´åœ°ç†åˆ†æ...");
            generateSpatialAnalysisMetrics(deliverClean, pickupClean, outputPath + "/spatial_analysis", spark);
            jobMonitor.logModuleCompletion(jobId, "ç©ºé—´åœ°ç†åˆ†æ", totalProcessedRecords);

            // 3. è¿è¥æ•ˆç‡åˆ†æ
            System.out.println("âš¡ æ‰§è¡Œè¿è¥æ•ˆç‡åˆ†æ...");
            generateOperationalEfficiencyMetrics(deliverClean, pickupClean, outputPath + "/operational_efficiency", spark);
            jobMonitor.logModuleCompletion(jobId, "è¿è¥æ•ˆç‡åˆ†æ", totalProcessedRecords);

            // 4. é¢„æµ‹åˆ†ææ•°æ®
            System.out.println("ğŸ”® ç”Ÿæˆé¢„æµ‹åˆ†ææ•°æ®...");
            generatePredictiveAnalysisData(deliverClean, pickupClean, outputPath + "/predictive_data", spark);
            jobMonitor.logModuleCompletion(jobId, "é¢„æµ‹åˆ†ææ•°æ®", totalProcessedRecords);

            // 5. æˆæœ¬æ•ˆç›Šåˆ†æ
            System.out.println("ğŸ’° æ‰§è¡Œæˆæœ¬æ•ˆç›Šåˆ†æ...");
            generateCostAnalysisMetrics(deliverClean, pickupClean, outputPath + "/cost_analysis", spark);
            jobMonitor.logModuleCompletion(jobId, "æˆæœ¬æ•ˆç›Šåˆ†æ", totalProcessedRecords);

            // 6. KPIç›‘æ§æŒ‡æ ‡
            System.out.println("ğŸ“ˆ ç”ŸæˆKPIç›‘æ§æŒ‡æ ‡...");
            generateKPIMetrics(deliverClean, pickupClean, outputPath + "/kpi_metrics", spark);
            jobMonitor.logModuleCompletion(jobId, "KPIç›‘æ§æŒ‡æ ‡", totalProcessedRecords);

            // 7. å¼‚å¸¸æ£€æµ‹åˆ†æ
            System.out.println("ğŸ” æ‰§è¡Œå¼‚å¸¸æ£€æµ‹åˆ†æ...");
            generateAnomalyDetectionMetrics(deliverClean, pickupClean, outputPath + "/anomaly_detection", spark);
            jobMonitor.logModuleCompletion(jobId, "å¼‚å¸¸æ£€æµ‹åˆ†æ", totalProcessedRecords);

            // 8. ç»¼åˆæŠ¥è¡¨æ•°æ®
            System.out.println("ğŸ“‹ ç”Ÿæˆç»¼åˆæŠ¥è¡¨æ•°æ®...");
            generateComprehensiveReports(deliverClean, pickupClean, outputPath + "/comprehensive_reports", spark);
            jobMonitor.logModuleCompletion(jobId, "ç»¼åˆæŠ¥è¡¨æ•°æ®", totalProcessedRecords);

            // ğŸ‰ ä½œä¸šæˆåŠŸå®Œæˆ
            success = true;
            System.out.println("\n=== æ‰€æœ‰åˆ†ææ¨¡å—æ‰§è¡Œå®Œæˆ ===");
            System.out.println("å¢å¼ºç‰ˆç‰©æµæ•°æ®åˆ†ææˆåŠŸå®Œæˆ!");
            System.out.println("ç»“æœå·²ä¿å­˜åˆ°: " + outputPath);
            System.out.println("æ•°æ®å·²åŒæ­¥åˆ°MySQLæ•°æ®åº“");

        } catch (Exception e) {
            errorMessage = "åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: " + e.getMessage();
            System.err.println(errorMessage);
            e.printStackTrace();
            success = false;
        } finally {
            // ğŸ“Š å®Œæˆä½œä¸šç›‘æ§
            jobMonitor.completeJobTracking(jobId, success, totalProcessedRecords, errorMessage);

            spark.stop();

            if (!success) {
                System.exit(1);
            }
        }
    }

    /**
     * åˆå§‹åŒ–MySQLé…ç½®
     */
    private static void initMySQLConfig() {
        mysqlProps = new Properties();
        mysqlProps.setProperty("user", "root");
        mysqlProps.setProperty("password", "root");
        mysqlProps.setProperty("driver", "com.mysql.cj.jdbc.Driver");
        mysqlUrl = "jdbc:mysql://localhost:3306/logistics_db";
    }

    /**
     * æ¸…æ´—å’Œè½¬æ¢é…é€æ•°æ®
     * deliveræ•°æ®æ ¼å¼ï¼šorder_id, region_id, city, courier_id, lng, lat, aoi_id, aoi_type,
     * accept_time, accept_gps_time, accept_gps_lng, accept_gps_lat,
     * delivery_time, delivery_gps_time, delivery_gps_lng, delivery_gps_lat, ds
     */

    private static Dataset<Row> cleanAndTransformDeliveryData(Dataset<Row> deliverRaw) {
        return deliverRaw
                .filter(col("order_id").isNotNull())
                .filter(col("city").isNotNull())
                .filter(col("courier_id").isNotNull())
                .filter(col("delivery_time").isNotNull())
                // åœ¨åŸæ—¶é—´å­—ç¬¦ä¸²å‰æ·»åŠ å¹´ä»½
                .withColumn("delivery_time_with_year", concat(lit("2025-"), col("delivery_time")))
                .withColumn("accept_time_with_year", concat(lit("2025-"), col("accept_time")))
                .withColumn("delivery_gps_time_with_year", concat(lit("2025-"), col("delivery_gps_time")))
                .withColumn("accept_gps_time_with_year", concat(lit("2025-"), col("accept_gps_time")))
                // ä½¿ç”¨å®Œæ•´çš„æ—¶é—´æ ¼å¼è¿›è¡Œè½¬æ¢
                .withColumn("delivery_time", to_timestamp(col("delivery_time_with_year"), "yyyy-MM-dd HH:mm:ss"))
                .withColumn("accept_time", to_timestamp(col("accept_time_with_year"), "yyyy-MM-dd HH:mm:ss"))
                .withColumn("delivery_gps_time", to_timestamp(col("delivery_gps_time_with_year"), "yyyy-MM-dd HH:mm:ss"))
                .withColumn("accept_gps_time", to_timestamp(col("accept_gps_time_with_year"), "yyyy-MM-dd HH:mm:ss"))
                // åˆ é™¤ä¸´æ—¶åˆ—
                .drop("delivery_time_with_year", "accept_time_with_year",
                        "delivery_gps_time_with_year", "accept_gps_time_with_year")
                .withColumn("date", to_date(col("delivery_time")))
                .withColumn("hour", hour(col("delivery_time")))
                // è®¡ç®—é…é€æ—¶é•¿ï¼ˆä»æ¥å•åˆ°å®Œæˆé…é€ï¼‰
                .withColumn("delivery_duration_hours",
                        (unix_timestamp(col("delivery_time")).minus(unix_timestamp(col("accept_time")))).divide(3600))
                // è®¡ç®—é…é€è·ç¦»ï¼ˆGPSåæ ‡ä¹‹é—´çš„è·ç¦»ï¼‰
                .withColumn("delivery_distance_km",
                        calculateDistance(col("accept_gps_lng"), col("accept_gps_lat"),
                                col("delivery_gps_lng"), col("delivery_gps_lat")))
                .filter(col("delivery_duration_hours").between(0, 72)) // è¿‡æ»¤å¼‚å¸¸æ—¶é—´
                .filter(col("delivery_distance_km").between(0, 500));  // è¿‡æ»¤å¼‚å¸¸è·ç¦»
    }

    /**
     * æ¸…æ´—å’Œè½¬æ¢å–ä»¶æ•°æ®
     * pickupæ•°æ®æ ¼å¼ï¼šorder_id, region_id, city, courier_id, accept_time, time_window_start, time_window_end,
     * lng, lat, aoi_id, aoi_type, pickup_time, pickup_gps_time, pickup_gps_lng, pickup_gps_lat,
     * accept_gps_time, accept_gps_lng, accept_gps_lat, ds
     */
    private static Dataset<Row> cleanAndTransformPickupData(Dataset<Row> pickupRaw) {
        return pickupRaw
                .filter(col("order_id").isNotNull())
                .filter(col("city").isNotNull())
                .filter(col("courier_id").isNotNull())
                .filter(col("pickup_time").isNotNull())
                // åœ¨åŸæ—¶é—´å­—ç¬¦ä¸²å‰æ·»åŠ å¹´ä»½ï¼Œå°†"05-18 08:16:00"è½¬æ¢ä¸º"2025-05-18 08:16:00"
                .withColumn("pickup_time_with_year", concat(lit("2025-"), col("pickup_time")))
                .withColumn("accept_time_with_year", concat(lit("2025-"), col("accept_time")))
                .withColumn("pickup_gps_time_with_year", concat(lit("2025-"), col("pickup_gps_time")))
                .withColumn("accept_gps_time_with_year", concat(lit("2025-"), col("accept_gps_time")))
                .withColumn("time_window_start_with_year", concat(lit("2025-"), col("time_window_start")))
                .withColumn("time_window_end_with_year", concat(lit("2025-"), col("time_window_end")))
                // ä½¿ç”¨å®Œæ•´çš„æ—¶é—´æ ¼å¼è¿›è¡Œè½¬æ¢
                .withColumn("pickup_time", to_timestamp(col("pickup_time_with_year"), "yyyy-MM-dd HH:mm:ss"))
                .withColumn("accept_time", to_timestamp(col("accept_time_with_year"), "yyyy-MM-dd HH:mm:ss"))
                .withColumn("pickup_gps_time", to_timestamp(col("pickup_gps_time_with_year"), "yyyy-MM-dd HH:mm:ss"))
                .withColumn("accept_gps_time", to_timestamp(col("accept_gps_time_with_year"), "yyyy-MM-dd HH:mm:ss"))
                .withColumn("time_window_start", to_timestamp(col("time_window_start_with_year"), "yyyy-MM-dd HH:mm:ss"))
                .withColumn("time_window_end", to_timestamp(col("time_window_end_with_year"), "yyyy-MM-dd HH:mm:ss"))
                // åˆ é™¤ä¸´æ—¶åˆ—
                .drop("pickup_time_with_year", "accept_time_with_year",
                        "pickup_gps_time_with_year", "accept_gps_time_with_year",
                        "time_window_start_with_year", "time_window_end_with_year")
                .withColumn("date", to_date(col("pickup_time")))
                .withColumn("hour", hour(col("pickup_time")))
                // è®¡ç®—å–ä»¶æ—¶é•¿ï¼ˆä»æ¥å•åˆ°å®Œæˆå–ä»¶ï¼‰
                .withColumn("pickup_duration_hours",
                        (unix_timestamp(col("pickup_time")).minus(unix_timestamp(col("accept_time")))).divide(3600))
                // è®¡ç®—å–ä»¶è·ç¦»
                .withColumn("pickup_distance_km",
                        calculateDistance(col("accept_gps_lng"), col("accept_gps_lat"),
                                col("pickup_gps_lng"), col("pickup_gps_lat")))
                // æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´çª—å£å†…å®Œæˆ
                .withColumn("within_time_window",
                        col("pickup_time").between(col("time_window_start"), col("time_window_end")))
                .filter(col("pickup_duration_hours").between(0, 48))
                .filter(col("pickup_distance_km").between(0, 300));
    }

    /**
     * è®¡ç®—ä¸¤ç‚¹é—´è·ç¦»ï¼ˆç®€åŒ–ç‰ˆï¼Œä½¿ç”¨haversineå…¬å¼ï¼‰
     */
    private static org.apache.spark.sql.Column calculateDistance(
            org.apache.spark.sql.Column lng1, org.apache.spark.sql.Column lat1,
            org.apache.spark.sql.Column lng2, org.apache.spark.sql.Column lat2) {
        // ç®€åŒ–è·ç¦»è®¡ç®—ï¼Œå®é™…é¡¹ç›®ä¸­å»ºè®®ä½¿ç”¨æ›´ç²¾ç¡®çš„å…¬å¼
        return sqrt(
                pow(lng2.minus(lng1).multiply(lit(111.0)), lit(2))
                        .plus(pow(lat2.minus(lat1).multiply(lit(111.0)), lit(2)))
        );
    }

    /**
     * 1. æ—¶é—´æ•ˆç‡åˆ†æ - ä¿®æ­£å­—æ®µåç§°
     */
    private static void generateTimeEfficiencyMetrics(Dataset<Row> delivery, Dataset<Row> pickup, String outputPath, SparkSession spark) {
        try {
            // é…é€æ—¶é—´æ•ˆç‡åˆ†æ
            Dataset<Row> deliveryTimeMetrics = delivery
                    .groupBy("city", "date", "hour")
                    .agg(
                            count("order_id").alias("total_deliveries"),
                            avg("delivery_duration_hours").alias("avg_delivery_time"),
                            percentile_approx(col("delivery_duration_hours"), lit(0.5), lit(10000)).alias("median_delivery_time"),
                            percentile_approx(col("delivery_duration_hours"), lit(0.95), lit(10000)).alias("p95_delivery_time"),
                            sum(when(col("delivery_duration_hours").leq(2), 1).otherwise(0)).alias("fast_deliveries"),
                            sum(when(col("delivery_duration_hours").between(2, 8), 1).otherwise(0)).alias("normal_deliveries"),
                            sum(when(col("delivery_duration_hours").gt(8), 1).otherwise(0)).alias("slow_deliveries")
                    )
                    .withColumn("fast_delivery_rate", col("fast_deliveries").divide(col("total_deliveries")))
                    .withColumn("slow_delivery_rate", col("slow_deliveries").divide(col("total_deliveries")));

            // å–ä»¶æ—¶é—´æ•ˆç‡åˆ†æ
            Dataset<Row> pickupTimeMetrics = pickup
                    .groupBy("city", "date", "hour")
                    .agg(
                            count("order_id").alias("total_pickups"),
                            avg("pickup_duration_hours").alias("avg_pickup_time"),
                            percentile_approx(col("pickup_duration_hours"), lit(0.5), lit(10000)).alias("median_pickup_time"),
                            percentile_approx(col("pickup_duration_hours"), lit(0.95), lit(10000)).alias("p95_pickup_time"),
                            sum(when(col("pickup_duration_hours").leq(1), 1).otherwise(0)).alias("fast_pickups"),
                            sum(when(col("pickup_duration_hours").between(1, 4), 1).otherwise(0)).alias("normal_pickups"),
                            sum(when(col("pickup_duration_hours").gt(4), 1).otherwise(0)).alias("slow_pickups"),
                            // æ–°å¢ï¼šæ—¶é—´çª—å£éµå®ˆç‡
                            sum(when(col("within_time_window").equalTo(true), 1).otherwise(0)).alias("on_time_pickups")
                    )
                    .withColumn("fast_pickup_rate", col("fast_pickups").divide(col("total_pickups")))
                    .withColumn("slow_pickup_rate", col("slow_pickups").divide(col("total_pickups")))
                    .withColumn("on_time_pickup_rate", col("on_time_pickups").divide(col("total_pickups")));

            // åˆå¹¶é…é€å’Œå–ä»¶æ•°æ®
            Dataset<Row> combinedTimeMetrics = deliveryTimeMetrics
                    .join(pickupTimeMetrics,
                            JavaConverters.asScalaIteratorConverter(
                                    Arrays.asList("city", "date", "hour").iterator()
                            ).asScala().toSeq(),
                            "full_outer")
                    .na().fill(0);

            // ä¿å­˜åˆ°HDFS
            deliveryTimeMetrics.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/delivery_time_metrics");

            pickupTimeMetrics.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/pickup_time_metrics");

            // å†™å…¥MySQL
            writeTimeEfficiencyToMySQL(combinedTimeMetrics);

            System.out.println("æ—¶é—´æ•ˆç‡åˆ†æå®Œæˆï¼ˆHDFS + MySQLï¼‰");

        } catch (Exception e) {
            System.err.println("æ—¶é—´æ•ˆç‡åˆ†æå¤±è´¥: " + e.getMessage());
            throw e;
        }
    }

    /**
     * 2. ç©ºé—´åœ°ç†åˆ†æ - ä¿®æ­£å­—æ®µåç§°
     */
    private static void generateSpatialAnalysisMetrics(Dataset<Row> delivery, Dataset<Row> pickup, String outputPath, SparkSession spark) {
        try {
            // é…é€ç©ºé—´åˆ†æ - ä½¿ç”¨delivery_gps_lng/lat
            Dataset<Row> deliverySpatialMetrics = delivery
                    .filter(col("delivery_gps_lng").isNotNull().and(col("delivery_gps_lat").isNotNull()))
                    .withColumn("lng_grid", floor(col("delivery_gps_lng").multiply(100)).divide(100))
                    .withColumn("lat_grid", floor(col("delivery_gps_lat").multiply(100)).divide(100))
                    .groupBy("city", "date", "lng_grid", "lat_grid")
                    .agg(
                            count("order_id").alias("delivery_count"),
                            countDistinct("courier_id").alias("unique_couriers"),
                            avg("delivery_duration_hours").alias("avg_delivery_time"),
                            avg("delivery_distance_km").alias("avg_delivery_distance")
                    )
                    .withColumn("delivery_density", col("delivery_count").divide(0.01 * 0.01));

            // å–ä»¶ç©ºé—´åˆ†æ - ä½¿ç”¨pickup_gps_lng/lat
            Dataset<Row> pickupSpatialMetrics = pickup
                    .filter(col("pickup_gps_lng").isNotNull().and(col("pickup_gps_lat").isNotNull()))
                    .withColumn("lng_grid", floor(col("pickup_gps_lng").multiply(100)).divide(100))
                    .withColumn("lat_grid", floor(col("pickup_gps_lat").multiply(100)).divide(100))
                    .groupBy("city", "date", "lng_grid", "lat_grid")
                    .agg(
                            count("order_id").alias("pickup_count"),
                            countDistinct("courier_id").alias("unique_couriers"),
                            avg("pickup_duration_hours").alias("avg_pickup_time"),
                            avg("pickup_distance_km").alias("avg_pickup_distance")
                    )
                    .withColumn("pickup_density", col("pickup_count").divide(0.01 * 0.01));

            // AOIåŒºåŸŸè¦†ç›–åˆ†æ
            Dataset<Row> regionCoverageMetrics = delivery
                    .groupBy("city", "date", "aoi_id", "aoi_type")
                    .agg(
                            count("order_id").alias("orders_in_aoi"),
                            countDistinct("courier_id").alias("couriers_in_aoi"),
                            avg("delivery_duration_hours").alias("avg_aoi_delivery_time")
                    )
                    .withColumn("orders_per_courier", col("orders_in_aoi").divide(col("couriers_in_aoi")));

            // ä¿å­˜åˆ°HDFS
            deliverySpatialMetrics.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/delivery_spatial_metrics");

            pickupSpatialMetrics.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/pickup_spatial_metrics");

            regionCoverageMetrics.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/region_coverage_metrics");

            // å†™å…¥MySQL
            writeSpatialAnalysisToMySQL(deliverySpatialMetrics, regionCoverageMetrics);

            System.out.println("ç©ºé—´åœ°ç†åˆ†æå®Œæˆï¼ˆHDFS + MySQLï¼‰");

        } catch (Exception e) {
            System.err.println("ç©ºé—´åœ°ç†åˆ†æå¤±è´¥: " + e.getMessage());
            throw e;
        }
    }

    /**
     * 3. è¿è¥æ•ˆç‡åˆ†æ - ä¿®æ­£å­—æ®µåç§°
     */
    private static void generateOperationalEfficiencyMetrics(Dataset<Row> delivery, Dataset<Row> pickup, String outputPath, SparkSession spark) {
        try {
            // å¿«é€’å‘˜æ•ˆç‡åˆ†æ
            Dataset<Row> courierEfficiencyMetrics = delivery
                    .groupBy("city", "region_id", "courier_id", "date")
                    .agg(
                            count("order_id").alias("total_orders"),
                            countDistinct("aoi_id").alias("unique_aoi_served"),
                            sum("delivery_distance_km").alias("total_distance"),
                            sum("delivery_duration_hours").alias("total_working_hours"),
                            avg("delivery_duration_hours").alias("avg_delivery_time")
                    )
                    .withColumn("orders_per_hour", col("total_orders").divide(col("total_working_hours")))
                    .withColumn("distance_per_order", col("total_distance").divide(col("total_orders")))
                    .withColumn("efficiency_score",
                            col("orders_per_hour").multiply(0.6)
                                    .plus(lit(1).divide(col("avg_delivery_time")).multiply(0.4)))
                    .filter(col("total_working_hours").gt(0));

            // åŒºåŸŸè´Ÿè½½åˆ†æ
            Dataset<Row> regionLoadMetrics = delivery
                    .groupBy("city", "region_id", "date")
                    .agg(
                            count("order_id").alias("total_region_orders"),
                            countDistinct("courier_id").alias("active_couriers"),
                            countDistinct("aoi_id").alias("served_aois"),
                            avg("delivery_duration_hours").alias("avg_region_delivery_time")
                    )
                    .withColumn("orders_per_courier", col("total_region_orders").divide(col("active_couriers")))
                    .withColumn("orders_per_aoi", col("total_region_orders").divide(col("served_aois")))
                    .withColumn("region_load_score",
                            col("orders_per_courier").multiply(0.6).plus(col("orders_per_aoi").multiply(0.4)));

            // ä¿å­˜åˆ°HDFS
            courierEfficiencyMetrics.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/courier_efficiency_metrics");

            regionLoadMetrics.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/region_load_metrics");

            // å†™å…¥MySQL
            writeOperationalEfficiencyToMySQL(courierEfficiencyMetrics, regionLoadMetrics);

            System.out.println("è¿è¥æ•ˆç‡åˆ†æå®Œæˆï¼ˆHDFS + MySQLï¼‰");

        } catch (Exception e) {
            System.err.println("è¿è¥æ•ˆç‡åˆ†æå¤±è´¥: " + e.getMessage());
            throw e;
        }
    }

    /**
     * 4. é¢„æµ‹åˆ†ææ•°æ®ç”Ÿæˆ - ä¿®æ­£æ—¶é—´åºåˆ—åˆ†æ
     */
    private static void generatePredictiveAnalysisData(Dataset<Row> delivery, Dataset<Row> pickup, String outputPath, SparkSession spark) {
        try {
            // æ—¶é—´åºåˆ—è¶‹åŠ¿æ•°æ® - åŸºäºdså­—æ®µè¿›è¡Œæ—¶é—´åºåˆ—åˆ†æ
            Dataset<Row> timeSeriesTrends = delivery
                    .withColumn("ds_date", to_date(col("ds"), "MMdd")) // dsæ ¼å¼ä¸º518 -> 5/18
                    .groupBy("city", "ds_date", "hour")
                    .agg(
                            count("order_id").alias("order_volume"),
                            countDistinct("courier_id").alias("courier_count"),
                            avg("delivery_duration_hours").alias("avg_duration"),
                            sum("delivery_distance_km").alias("total_distance")
                    )
                    .withColumn("volume_trend", lag("order_volume", 1).over(
                            Window.partitionBy("city").orderBy(
                                    unix_timestamp(col("ds_date")).plus(col("hour").multiply(3600))
                            )))
                    .withColumn("efficiency_score",
                            col("order_volume").divide(col("courier_count").multiply(col("avg_duration"))))
                    .withColumn("data_type", lit("HOURLY"))
                    .withColumn("region_id", lit(null).cast("string"));

            // å®¹é‡è§„åˆ’æ•°æ®
            Dataset<Row> capacityPlanningData = delivery
                    .withColumn("ds_date", to_date(col("ds"), "MMdd"))
                    .groupBy("city", "region_id", "ds_date")
                    .agg(
                            count("order_id").alias("daily_orders"),
                            countDistinct("courier_id").alias("required_couriers"),
                            max("hour").alias("peak_hour"),
                            sum("delivery_distance_km").alias("total_daily_distance")
                    )
                    .withColumn("orders_per_courier_day", col("daily_orders").divide(col("required_couriers")))
                    .withColumn("capacity_utilization",
                            when(col("orders_per_courier_day").gt(30), lit("HIGH"))
                                    .when(col("orders_per_courier_day").gt(20), lit("MEDIUM"))
                                    .otherwise(lit("LOW")))
                    .withColumn("data_type", lit("DAILY"));

            // ä¿å­˜åˆ°HDFS
            timeSeriesTrends.write()
                    .mode("overwrite")
                    .partitionBy("city")
                    .parquet(outputPath + "/time_series_trends");

            capacityPlanningData.write()
                    .mode("overwrite")
                    .partitionBy("city")
                    .parquet(outputPath + "/capacity_planning_data");

            // å†™å…¥MySQL
            writePredictiveAnalysisToMySQL(timeSeriesTrends, capacityPlanningData);

            System.out.println("é¢„æµ‹åˆ†ææ•°æ®ç”Ÿæˆå®Œæˆï¼ˆHDFS + MySQLï¼‰");

        } catch (Exception e) {
            System.err.println("é¢„æµ‹åˆ†ææ•°æ®ç”Ÿæˆå¤±è´¥: " + e.getMessage());
            throw e;
        }
    }

    /**
     * 5. æˆæœ¬æ•ˆç›Šåˆ†æ - æ ¹æ®å®é™…æ•°æ®ç»“æ„è°ƒæ•´
     */
    private static void generateCostAnalysisMetrics(Dataset<Row> delivery, Dataset<Row> pickup, String outputPath, SparkSession spark) {
        try {
            // æˆæœ¬ç»“æ„åˆ†æ
            Dataset<Row> costStructureMetrics = delivery
                    .withColumn("fuel_cost", col("delivery_distance_km").multiply(0.8)) // æ¯å…¬é‡Œ0.8å…ƒæ²¹è´¹
                    .withColumn("time_cost", col("delivery_duration_hours").multiply(25)) // æ¯å°æ—¶25å…ƒäººå·¥æˆæœ¬
                    .withColumn("total_delivery_cost", col("fuel_cost").plus(col("time_cost")))
                    .groupBy("city", "region_id", "date")
                    .agg(
                            sum("total_delivery_cost").alias("total_cost"),
                            sum("fuel_cost").alias("total_fuel_cost"),
                            sum("time_cost").alias("total_time_cost"),
                            count("order_id").alias("total_orders"),
                            sum("delivery_distance_km").alias("total_distance")
                    )
                    .withColumn("cost_per_order", col("total_cost").divide(col("total_orders")))
                    .withColumn("cost_per_km", col("total_cost").divide(col("total_distance")))
                    .withColumn("fuel_cost_ratio", col("total_fuel_cost").divide(col("total_cost")))
                    .withColumn("analysis_type", lit("REGION"));

            // æ•ˆç›Šè¯„ä¼°
            Dataset<Row> efficiencyROI = delivery
                    .groupBy("city", "courier_id", "date")
                    .agg(
                            count("order_id").alias("completed_orders"),
                            sum("delivery_distance_km").alias("distance_covered"),
                            sum("delivery_duration_hours").alias("working_hours")
                    )
                    .withColumn("productivity_score",
                            col("completed_orders").divide(col("working_hours")))
                    .withColumn("efficiency_rating",
                            when(col("productivity_score").gt(8), lit("EXCELLENT"))
                                    .when(col("productivity_score").gt(5), lit("GOOD"))
                                    .when(col("productivity_score").gt(3), lit("AVERAGE"))
                                    .otherwise(lit("NEEDS_IMPROVEMENT")))
                    .withColumn("analysis_type", lit("COURIER"));

            // ä¿å­˜åˆ°HDFS
            costStructureMetrics.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/cost_structure_metrics");

            efficiencyROI.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/efficiency_roi_metrics");

            // å†™å…¥MySQL
            writeCostAnalysisToMySQL(costStructureMetrics, efficiencyROI);

            System.out.println("æˆæœ¬æ•ˆç›Šåˆ†æå®Œæˆï¼ˆHDFS + MySQLï¼‰");

        } catch (Exception e) {
            System.err.println("æˆæœ¬æ•ˆç›Šåˆ†æå¤±è´¥: " + e.getMessage());
            throw e;
        }
    }

    /**
     * 6. KPIç›‘æ§æŒ‡æ ‡ç”Ÿæˆ - è°ƒæ•´KPIè®¡ç®—é€»è¾‘
     */
    private static void generateKPIMetrics(Dataset<Row> delivery, Dataset<Row> pickup, String outputPath, SparkSession spark) {
        try {
            // æ ¸å¿ƒKPIæŒ‡æ ‡
            Dataset<Row> coreKPIs = delivery
                    .groupBy("city", "date", "hour")
                    .agg(
                            count("order_id").alias("total_orders"),
                            countDistinct("courier_id").alias("active_couriers"),
                            countDistinct("aoi_id").alias("coverage_aois"),
                            avg("delivery_duration_hours").alias("avg_delivery_time"),
                            sum(when(col("delivery_duration_hours").leq(2), 1).otherwise(0)).alias("fast_deliveries")
                    )
                    .withColumn("orders_per_courier", col("total_orders").divide(col("active_couriers")))
                    .withColumn("orders_per_aoi", col("total_orders").divide(col("coverage_aois")))
                    .withColumn("fast_delivery_rate", col("fast_deliveries").divide(col("total_orders")))
                    .withColumn("efficiency_score",
                            col("orders_per_courier").multiply(0.4)
                                    .plus(col("fast_delivery_rate").multiply(0.6)))
                    .withColumn("kpi_timestamp", current_timestamp());

            // æœåŠ¡è´¨é‡KPI - åŒ…å«å–ä»¶æœåŠ¡
            Dataset<Row> serviceQualityKPIs = pickup
                    .groupBy("city", "date", "hour")
                    .agg(
                            count("order_id").alias("total_pickups"),
                            sum(when(col("within_time_window").equalTo(true), 1).otherwise(0)).alias("on_time_pickups"),
                            avg("pickup_duration_hours").alias("avg_pickup_duration")
                    )
                    .withColumn("on_time_pickup_rate", col("on_time_pickups").divide(col("total_pickups")))
                    .withColumn("pickup_service_score", col("on_time_pickup_rate").multiply(100));

            // ä¿å­˜åˆ°HDFS
            coreKPIs.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/core_kpis");

            serviceQualityKPIs.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/service_quality_kpis");

            // å†™å…¥MySQLå®æ—¶KPIè¡¨
            writeKPIsToMySQL(coreKPIs, serviceQualityKPIs, spark);

            System.out.println("KPIç›‘æ§æŒ‡æ ‡ç”Ÿæˆå®Œæˆï¼ˆHDFS + MySQLï¼‰");

        } catch (Exception e) {
            System.err.println("KPIç›‘æ§æŒ‡æ ‡ç”Ÿæˆå¤±è´¥: " + e.getMessage());
            throw e;
        }
    }

    /**
     * 7. å¼‚å¸¸æ£€æµ‹åˆ†æ - è°ƒæ•´å¼‚å¸¸æ£€æµ‹é˜ˆå€¼
     */
    private static void generateAnomalyDetectionMetrics(Dataset<Row> delivery, Dataset<Row> pickup, String outputPath, SparkSession spark) {
        try {
            // é…é€æ—¶é—´å¼‚å¸¸æ£€æµ‹ - æ ¹æ®å®é™…æ•°æ®è°ƒæ•´é˜ˆå€¼
            Dataset<Row> deliveryTimeAnomalies = delivery
                    .withColumn("delivery_time_zscore",
                            (col("delivery_duration_hours").minus(lit(4.0))).divide(lit(3.0))) // å‡å€¼4å°æ—¶ï¼Œæ ‡å‡†å·®3å°æ—¶
                    .filter(abs(col("delivery_time_zscore")).gt(2))
                    .select("order_id", "city", "courier_id", "delivery_duration_hours", "delivery_time_zscore", "date")
                    .withColumn("anomaly_type", lit("DELIVERY_TIME_ANOMALY"))
                    .withColumn("anomaly_severity",
                            when(abs(col("delivery_time_zscore")).gt(3), lit("HIGH"))
                                    .when(abs(col("delivery_time_zscore")).gt(2.5), lit("MEDIUM"))
                                    .otherwise(lit("LOW")));

            // å–ä»¶æ—¶é—´å¼‚å¸¸æ£€æµ‹
            Dataset<Row> pickupTimeAnomalies = pickup
                    .withColumn("pickup_time_zscore",
                            (col("pickup_duration_hours").minus(lit(2.0))).divide(lit(1.5))) // å‡å€¼2å°æ—¶ï¼Œæ ‡å‡†å·®1.5å°æ—¶
                    .filter(abs(col("pickup_time_zscore")).gt(2))
                    .select("order_id", "city", "courier_id", "pickup_duration_hours", "pickup_time_zscore", "date")
                    .withColumn("anomaly_type", lit("PICKUP_TIME_ANOMALY"))
                    .withColumn("anomaly_severity",
                            when(abs(col("pickup_time_zscore")).gt(3), lit("HIGH"))
                                    .when(abs(col("pickup_time_zscore")).gt(2.5), lit("MEDIUM"))
                                    .otherwise(lit("LOW")));

            // è·ç¦»å¼‚å¸¸æ£€æµ‹
            Dataset<Row> distanceAnomalies = delivery
                    .withColumn("distance_zscore",
                            (col("delivery_distance_km").minus(lit(10))).divide(lit(8))) // å‡å€¼10kmï¼Œæ ‡å‡†å·®8km
                    .filter(abs(col("distance_zscore")).gt(2))
                    .select("order_id", "city", "courier_id", "delivery_distance_km", "distance_zscore", "date")
                    .withColumn("anomaly_type", lit("DISTANCE_ANOMALY"))
                    .withColumn("anomaly_severity",
                            when(abs(col("distance_zscore")).gt(3), lit("HIGH"))
                                    .when(abs(col("distance_zscore")).gt(2.5), lit("MEDIUM"))
                                    .otherwise(lit("LOW")));

            // ä¿å­˜åˆ°HDFS
            deliveryTimeAnomalies.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/delivery_time_anomalies");

            pickupTimeAnomalies.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/pickup_time_anomalies");

            distanceAnomalies.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/distance_anomalies");

            // å†™å…¥MySQLå¼‚å¸¸å‘Šè­¦è¡¨
            writeAnomaliesToMySQL(deliveryTimeAnomalies, pickupTimeAnomalies, distanceAnomalies, spark);

            System.out.println("å¼‚å¸¸æ£€æµ‹åˆ†æå®Œæˆï¼ˆHDFS + MySQLï¼‰");

        } catch (Exception e) {
            System.err.println("å¼‚å¸¸æ£€æµ‹åˆ†æå¤±è´¥: " + e.getMessage());
            throw e;
        }
    }

    /**
     * 8. ç»¼åˆæŠ¥è¡¨æ•°æ®ç”Ÿæˆ - æ ¹æ®æ•°æ®ç»“æ„è°ƒæ•´
     */
    private static void generateComprehensiveReports(Dataset<Row> delivery, Dataset<Row> pickup, String outputPath, SparkSession spark) {
        try {
            // æ—¥æŠ¥æ•°æ®
            Dataset<Row> dailyReports = delivery
                    .groupBy("city", "region_id", "date")
                    .agg(
                            count("order_id").alias("total_deliveries"),
                            countDistinct("courier_id").alias("active_couriers"),
                            countDistinct("aoi_id").alias("served_aois"),
                            avg("delivery_duration_hours").alias("avg_delivery_time"),
                            sum("delivery_distance_km").alias("total_distance"),
                            sum(when(col("delivery_duration_hours").leq(2), 1).otherwise(0)).alias("fast_deliveries")
                    )
                    .withColumn("avg_orders_per_courier", col("total_deliveries").divide(col("active_couriers")))
                    .withColumn("avg_distance_per_order", col("total_distance").divide(col("total_deliveries")))
                    .withColumn("fast_delivery_rate", col("fast_deliveries").divide(col("total_deliveries")))
                    .withColumn("report_type", lit("DAILY"))
                    .withColumn("generated_at", current_timestamp());

            // å–ä»¶æ—¥æŠ¥æ•°æ®
            Dataset<Row> pickupDailyReports = pickup
                    .groupBy("city", "region_id", "date")
                    .agg(
                            count("order_id").alias("total_pickups"),
                            countDistinct("courier_id").alias("active_pickup_couriers"),
                            avg("pickup_duration_hours").alias("avg_pickup_time"),
                            sum(when(col("within_time_window").equalTo(true), 1).otherwise(0)).alias("on_time_pickups")
                    )
                    .withColumn("on_time_pickup_rate", col("on_time_pickups").divide(col("total_pickups")))
                    .withColumn("report_type", lit("PICKUP_DAILY"))
                    .withColumn("generated_at", current_timestamp());

            // ä¿å­˜åˆ°HDFS
            dailyReports.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/daily_reports");

            pickupDailyReports.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/pickup_daily_reports");

            // å†™å…¥MySQL
            writeComprehensiveReportsToMySQL(dailyReports, pickupDailyReports);

            System.out.println("ç»¼åˆæŠ¥è¡¨æ•°æ®ç”Ÿæˆå®Œæˆï¼ˆHDFS + MySQLï¼‰");

        } catch (Exception e) {
            System.err.println("ç»¼åˆæŠ¥è¡¨æ•°æ®ç”Ÿæˆå¤±è´¥: " + e.getMessage());
            throw e;
        }
    }

    // ========== MySQLå†™å…¥æ–¹æ³•ï¼ˆä¿®æ­£å­—æ®µåŒ¹é…é—®é¢˜ï¼‰ ==========

    /**
     * å†™å…¥æ—¶é—´æ•ˆç‡æ•°æ®åˆ°MySQL
     */
    private static void writeTimeEfficiencyToMySQL(Dataset<Row> timeMetrics) {
        try {
            //Dataset<Row> recentData = timeMetrics
             //       .filter(col("date").geq(date_sub(current_date(), 7)));

            //recentData.write()
            timeMetrics.write()
                    .mode("overwrite")
                    .jdbc(mysqlUrl, "time_efficiency_metrics", mysqlProps);

            System.out.println("æ—¶é—´æ•ˆç‡æ•°æ®å·²å†™å…¥MySQL");

            System.out.println("å¾…å†™å…¥è®°å½•æ•°: " + timeMetrics.count());
        } catch (Exception e) {
            System.err.println("æ—¶é—´æ•ˆç‡æ•°æ®MySQLå†™å…¥å¤±è´¥: " + e.getMessage());
        }
    }

    /**
     * å†™å…¥ç©ºé—´åœ°ç†åˆ†ææ•°æ®åˆ°MySQL
     */
    private static void writeSpatialAnalysisToMySQL(Dataset<Row> spatialMetrics, Dataset<Row> regionMetrics) {
        try {
//            Dataset<Row> recentSpatialData = spatialMetrics
//                    .filter(col("date").geq(date_sub(current_date(), 7)));

            spatialMetrics.write()
                    .mode("overwrite")
                    .jdbc(mysqlUrl, "spatial_analysis_metrics", mysqlProps);

            System.out.println("ç©ºé—´åœ°ç†åˆ†ææ•°æ®å·²å†™å…¥MySQL");

        } catch (Exception e) {
            System.err.println("ç©ºé—´åœ°ç†åˆ†ææ•°æ®MySQLå†™å…¥å¤±è´¥: " + e.getMessage());
        }
    }

    /**
     * å†™å…¥è¿è¥æ•ˆç‡æ•°æ®åˆ°MySQL
     */
    private static void writeOperationalEfficiencyToMySQL(Dataset<Row> courierMetrics, Dataset<Row> regionMetrics) {
        try {
//            Dataset<Row> recentData = courierMetrics
//                    .filter(col("date").geq(date_sub(current_date(), 7)));

            courierMetrics.write()
                    .mode("overwrite")
                    .jdbc(mysqlUrl, "operational_efficiency_metrics", mysqlProps);

            System.out.println("âœ… è¿è¥æ•ˆç‡æ•°æ®å·²å†™å…¥MySQL");

        } catch (Exception e) {
            System.err.println("âš ï¸ è¿è¥æ•ˆç‡æ•°æ®MySQLå†™å…¥å¤±è´¥: " + e.getMessage());
        }
    }

    /**
     * å†™å…¥é¢„æµ‹åˆ†ææ•°æ®åˆ°MySQL
     */
    private static void writePredictiveAnalysisToMySQL(Dataset<Row> timeSeriesData, Dataset<Row> capacityData) {
        try {
            timeSeriesData.write()
                    .mode("overwrite")
                    .jdbc(mysqlUrl, "predictive_analysis_data", mysqlProps);

            System.out.println("âœ… é¢„æµ‹åˆ†ææ•°æ®å·²å†™å…¥MySQL");

        } catch (Exception e) {
            System.err.println("âš ï¸ é¢„æµ‹åˆ†ææ•°æ®MySQLå†™å…¥å¤±è´¥: " + e.getMessage());
        }
    }

    /**
     * å†™å…¥æˆæœ¬åˆ†ææ•°æ®åˆ°MySQL
     */
    private static void writeCostAnalysisToMySQL(Dataset<Row> costMetrics, Dataset<Row> efficiencyMetrics) {
        try {
            Dataset<Row> recentData = costMetrics
                    .filter(col("date").geq(date_sub(current_date(), 7)));

            recentData.write()
                    .mode("overwrite")
                    .jdbc(mysqlUrl, "cost_analysis_metrics", mysqlProps);

            System.out.println("âœ… æˆæœ¬åˆ†ææ•°æ®å·²å†™å…¥MySQL");

        } catch (Exception e) {
            System.err.println("âš ï¸ æˆæœ¬åˆ†ææ•°æ®MySQLå†™å…¥å¤±è´¥: " + e.getMessage());
        }
    }

    /**
     * å†™å…¥ç»¼åˆæŠ¥è¡¨æ•°æ®åˆ°MySQL
     */
    private static void writeComprehensiveReportsToMySQL(Dataset<Row> dailyReports, Dataset<Row> pickupReports) {
        try {
//            Dataset<Row> recentData = dailyReports
//                    .filter(col("date").geq(date_sub(current_date(), 30)));

            dailyReports.write()
                    .mode("overwrite")
                    .jdbc(mysqlUrl, "comprehensive_reports", mysqlProps);

            System.out.println("âœ… ç»¼åˆæŠ¥è¡¨æ•°æ®å·²å†™å…¥MySQL");

        } catch (Exception e) {
            System.err.println("âš ï¸ ç»¼åˆæŠ¥è¡¨æ•°æ®MySQLå†™å…¥å¤±è´¥: " + e.getMessage());
        }
    }

    /**
     * å†™å…¥KPIæ•°æ®åˆ°MySQL
     */
    private static void writeKPIsToMySQL(Dataset<Row> coreKPIs, Dataset<Row> serviceQualityKPIs, SparkSession spark) {
        try {
//            Dataset<Row> recentKPIs = coreKPIs
//                    .filter(col("date").geq(date_sub(current_date(), 7)));

            coreKPIs.write()
                    .mode("overwrite")
                    .jdbc(mysqlUrl, "realtime_kpi", mysqlProps);

            System.out.println("âœ… KPIæ•°æ®å·²å†™å…¥MySQL");

        } catch (Exception e) {
            System.err.println("âš ï¸ KPIæ•°æ®MySQLå†™å…¥å¤±è´¥: " + e.getMessage());
        }
    }

    /**
     * å†™å…¥å¼‚å¸¸æ•°æ®åˆ°MySQL - ä¿®æ­£å‚æ•°
     */
    private static void writeAnomaliesToMySQL(Dataset<Row> deliveryTimeAnomalies, Dataset<Row> pickupTimeAnomalies, Dataset<Row> distanceAnomalies, SparkSession spark) {
        try {
            // ç»Ÿä¸€å¼‚å¸¸æ•°æ®æ ¼å¼
            Dataset<Row> allAnomalies = deliveryTimeAnomalies
                    .select(
                            col("order_id"), col("city"), col("courier_id"),
                            col("anomaly_type"), col("anomaly_severity"), col("date")
                    )
                    .union(pickupTimeAnomalies.select(
                            col("order_id"), col("city"), col("courier_id"),
                            col("anomaly_type"), col("anomaly_severity"), col("date")
                    ))
                    .union(distanceAnomalies.select(
                            col("order_id"), col("city"), col("courier_id"),
                            col("anomaly_type"), col("anomaly_severity"), col("date")
                    ));

            allAnomalies.write()
                    .mode("append")
                    .jdbc(mysqlUrl, "anomaly_alerts", mysqlProps);

            System.out.println("âœ… å¼‚å¸¸å‘Šè­¦æ•°æ®å·²å†™å…¥MySQL");

        } catch (Exception e) {
            System.err.println("âš ï¸ å¼‚å¸¸æ•°æ®MySQLå†™å…¥å¤±è´¥: " + e.getMessage());
        }
    }
}