package com.logistics.spark;

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.expressions.Window;
import org.apache.spark.sql.functions;
import org.apache.spark.sql.types.DataTypes;
import static org.apache.spark.sql.functions.*;

import java.util.Properties;

/**
 * å¢å¼ºç‰ˆåŸå¸‚ç‰©æµæ•°æ®åˆ†æç³»ç»Ÿ
 *
 * åŠŸèƒ½æ¨¡å—ï¼š
 * 1. æ—¶é—´æ•ˆç‡åˆ†æ - é…é€æ—¶æ•ˆã€å–ä»¶æ—¶æ•ˆåˆ†æ
 * 2. ç©ºé—´åœ°ç†åˆ†æ - çƒ­åŠ›å›¾ã€é…é€å¯†åº¦ã€åœ°ç†è¦†ç›–
 * 3. è¿è¥æ•ˆç‡åˆ†æ - å¿«é€’å‘˜æ•ˆç‡ã€åŒºåŸŸè´Ÿè½½åˆ†æ
 * 4. é¢„æµ‹åˆ†ææ•°æ® - è¶‹åŠ¿é¢„æµ‹ã€å®¹é‡è§„åˆ’æ•°æ®
 * 5. æˆæœ¬æ•ˆç›Šåˆ†æ - æˆæœ¬ç»“æ„ã€æ•ˆç›Šè¯„ä¼°
 * 6. KPIç›‘æ§æŒ‡æ ‡ - æ ¸å¿ƒæŒ‡æ ‡ã€æœåŠ¡è´¨é‡ç›‘æ§
 * 7. å¼‚å¸¸æ£€æµ‹åˆ†æ - æ—¶é—´å¼‚å¸¸ã€è·ç¦»å¼‚å¸¸æ£€æµ‹
 * 8. ç»¼åˆæŠ¥è¡¨æ•°æ® - æ—¥æŠ¥ã€å‘¨æŠ¥ã€æœˆæŠ¥æ•°æ®
 *
 * åŒæ—¶æ”¯æŒHDFSå­˜å‚¨å’ŒMySQLå®æ—¶æ•°æ®å†™å…¥
 */
public class EnhancedCityLogisticsAnalysis {

    public static void main(String[] args) {
        if (args.length != 3) {
            System.err.println("ä½¿ç”¨æ–¹æ³•: EnhancedCityLogisticsAnalysis <deliver_path> <pickup_path> <output_path>");
            System.err.println("ç¤ºä¾‹: spark-submit --class com.logistics.spark.EnhancedCityLogisticsAnalysis " +
                    "app.jar hdfs://namenode:9000/data/deliver/* hdfs://namenode:9000/data/pickup/* " +
                    "hdfs://namenode:9000/output/analysis");
            System.exit(1);
        }

        String deliverPath = args[0];
        String pickupPath = args[1];
        String outputPath = args[2];

        // åˆå§‹åŒ–Sparkä¼šè¯
        SparkSession spark = SparkSession.builder()
                .appName("Enhanced-City-Logistics-Analysis")
                .config("spark.sql.adaptive.enabled", "true")
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
                .config("spark.sql.adaptive.advisoryPartitionSizeInBytes", "128MB")
                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
                .getOrCreate();

        try {
            System.out.println("=== å¯åŠ¨å¢å¼ºç‰ˆç‰©æµæ•°æ®åˆ†æ ===");
            System.out.println("é…é€æ•°æ®è·¯å¾„: " + deliverPath);
            System.out.println("å–ä»¶æ•°æ®è·¯å¾„: " + pickupPath);
            System.out.println("è¾“å‡ºè·¯å¾„: " + outputPath);

            // åŠ è½½å’Œæ¸…æ´—æ•°æ®
            Dataset<Row> deliverRaw = spark.read()
                    .option("header", "true")
                    .option("inferSchema", "true")
                    .csv(deliverPath);

            Dataset<Row> pickupRaw = spark.read()
                    .option("header", "true")
                    .option("inferSchema", "true")
                    .csv(pickupPath);

            System.out.println("åŸå§‹æ•°æ®è®°å½•æ•° - é…é€: " + deliverRaw.count() + ", å–ä»¶: " + pickupRaw.count());

            // æ•°æ®æ¸…æ´—å’Œè½¬æ¢
            Dataset<Row> deliverClean = cleanAndTransformDeliveryData(deliverRaw);
            Dataset<Row> pickupClean = cleanAndTransformPickupData(pickupRaw);

            System.out.println("æ¸…æ´—åæ•°æ®è®°å½•æ•° - é…é€: " + deliverClean.count() + ", å–ä»¶: " + pickupClean.count());

            // ç¼“å­˜æ¸…æ´—åçš„æ•°æ®
            deliverClean.cache();
            pickupClean.cache();

            // æ‰§è¡Œ8ä¸ªåˆ†ææ¨¡å—
            System.out.println("\n=== å¼€å§‹æ‰§è¡Œåˆ†ææ¨¡å— ===");

            // 1. æ—¶é—´æ•ˆç‡åˆ†æ
            System.out.println("ğŸ“Š æ‰§è¡Œæ—¶é—´æ•ˆç‡åˆ†æ...");
            generateTimeEfficiencyMetrics(deliverClean, pickupClean, outputPath + "/time_efficiency", spark);

            // 2. ç©ºé—´åœ°ç†åˆ†æ
            System.out.println("ğŸ—ºï¸ æ‰§è¡Œç©ºé—´åœ°ç†åˆ†æ...");
            generateSpatialAnalysisMetrics(deliverClean, pickupClean, outputPath + "/spatial_analysis", spark);

            // 3. è¿è¥æ•ˆç‡åˆ†æ
            System.out.println("ğŸ“ˆ æ‰§è¡Œè¿è¥æ•ˆç‡åˆ†æ...");
            generateOperationalEfficiencyMetrics(deliverClean, pickupClean, outputPath + "/operational_efficiency", spark);

            // 4. é¢„æµ‹åˆ†ææ•°æ®
            System.out.println("ğŸ”® ç”Ÿæˆé¢„æµ‹åˆ†ææ•°æ®...");
            generatePredictiveAnalysisData(deliverClean, pickupClean, outputPath + "/predictive_data", spark);

            // 5. æˆæœ¬æ•ˆç›Šåˆ†æ
            System.out.println("ğŸ’° æ‰§è¡Œæˆæœ¬æ•ˆç›Šåˆ†æ...");
            generateCostAnalysisMetrics(deliverClean, pickupClean, outputPath + "/cost_analysis", spark);

            // 6. KPIç›‘æ§æŒ‡æ ‡
            System.out.println("ğŸ“± ç”ŸæˆKPIç›‘æ§æŒ‡æ ‡...");
            generateKPIMetrics(deliverClean, pickupClean, outputPath + "/kpi_metrics", spark);

            // 7. å¼‚å¸¸æ£€æµ‹åˆ†æ
            System.out.println("ğŸš¨ æ‰§è¡Œå¼‚å¸¸æ£€æµ‹åˆ†æ...");
            generateAnomalyDetectionMetrics(deliverClean, pickupClean, outputPath + "/anomaly_detection", spark);

            // 8. ç»¼åˆæŠ¥è¡¨æ•°æ®
            System.out.println("ğŸ“‹ ç”Ÿæˆç»¼åˆæŠ¥è¡¨æ•°æ®...");
            generateComprehensiveReports(deliverClean, pickupClean, outputPath + "/comprehensive_reports", spark);

            System.out.println("\n=== æ‰€æœ‰åˆ†ææ¨¡å—æ‰§è¡Œå®Œæˆ ===");
            System.out.println("ğŸ‰ å¢å¼ºç‰ˆç‰©æµæ•°æ®åˆ†ææˆåŠŸå®Œæˆ!");
            System.out.println("ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: " + outputPath);

        } catch (Exception e) {
            System.err.println("âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: " + e.getMessage());
            e.printStackTrace();
            System.exit(1);
        } finally {
            spark.stop();
        }
    }

    /**
     * æ¸…æ´—å’Œè½¬æ¢é…é€æ•°æ®
     */
    private static Dataset<Row> cleanAndTransformDeliveryData(Dataset<Row> deliverRaw) {
        return deliverRaw
                .filter(col("order_id").isNotNull())
                .filter(col("city").isNotNull())
                .filter(col("courier_id").isNotNull())
                .withColumn("deliver_time", to_timestamp(col("deliver_time"), "yyyy-MM-dd HH:mm:ss"))
                .withColumn("finish_time", to_timestamp(col("finish_time"), "yyyy-MM-dd HH:mm:ss"))
                .withColumn("date", to_date(col("deliver_time")))
                .withColumn("hour", hour(col("deliver_time")))
                .withColumn("delivery_duration_hours",
                        (unix_timestamp(col("finish_time")).minus(unix_timestamp(col("deliver_time")))).divide(3600))
                .withColumn("delivery_distance_km", col("delivery_distance").divide(1000))
                .filter(col("delivery_duration_hours").between(0, 72)) // è¿‡æ»¤å¼‚å¸¸æ—¶é—´
                .filter(col("delivery_distance_km").between(0, 500));  // è¿‡æ»¤å¼‚å¸¸è·ç¦»
    }

    /**
     * æ¸…æ´—å’Œè½¬æ¢å–ä»¶æ•°æ®
     */
    private static Dataset<Row> cleanAndTransformPickupData(Dataset<Row> pickupRaw) {
        return pickupRaw
                .filter(col("order_id").isNotNull())
                .filter(col("city").isNotNull())
                .filter(col("courier_id").isNotNull())
                .withColumn("pickup_time", to_timestamp(col("pickup_time"), "yyyy-MM-dd HH:mm:ss"))
                .withColumn("finish_time", to_timestamp(col("finish_time"), "yyyy-MM-dd HH:mm:ss"))
                .withColumn("date", to_date(col("pickup_time")))
                .withColumn("hour", hour(col("pickup_time")))
                .withColumn("pickup_duration_hours",
                        (unix_timestamp(col("finish_time")).minus(unix_timestamp(col("pickup_time")))).divide(3600))
                .withColumn("pickup_distance_km", col("pickup_distance").divide(1000))
                .filter(col("pickup_duration_hours").between(0, 48))
                .filter(col("pickup_distance_km").between(0, 300));
    }

    /**
     * 1. æ—¶é—´æ•ˆç‡åˆ†æ
     */
    private static void generateTimeEfficiencyMetrics(Dataset<Row> delivery, Dataset<Row> pickup, String outputPath, SparkSession spark) {
        try {
            // é…é€æ—¶é—´æ•ˆç‡åˆ†æ
            Dataset<Row> deliveryTimeMetrics = delivery
                    .groupBy("city", "date", "hour")
                    .agg(
                            count("order_id").alias("total_deliveries"),
                            avg("delivery_duration_hours").alias("avg_delivery_time"),
                            percentile_approx(col("delivery_duration_hours"), lit(0.5),lit(10000)).alias("median_delivery_time"),
                            percentile_approx(col("delivery_duration_hours"), lit(0.95),lit(10000)).alias("p95_delivery_time"),
                            sum(when(col("delivery_duration_hours").leq(1), 1).otherwise(0)).alias("fast_deliveries"),
                            sum(when(col("delivery_duration_hours").between(1, 4), 1).otherwise(0)).alias("normal_deliveries"),
                            sum(when(col("delivery_duration_hours").gt(4), 1).otherwise(0)).alias("slow_deliveries")
                    )
                    .withColumn("fast_delivery_rate", col("fast_deliveries").divide(col("total_deliveries")))
                    .withColumn("slow_delivery_rate", col("slow_deliveries").divide(col("total_deliveries")));

            // å–ä»¶æ—¶é—´æ•ˆç‡åˆ†æ
            Dataset<Row> pickupTimeMetrics = pickup
                    .groupBy("city", "date", "hour")
                    .agg(
                            count("order_id").alias("total_pickups"),
                            avg("pickup_duration_hours").alias("avg_pickup_time"),
                            percentile_approx(col("pickup_duration_hours"), lit(0.5),lit(10000)).alias("median_pickup_time"),
                            percentile_approx(col("pickup_duration_hours"), lit(0.95),lit(10000)).alias("p95_pickup_time"),
                            sum(when(col("pickup_duration_hours").leq(0.5), 1).otherwise(0)).alias("fast_pickups"),
                            sum(when(col("pickup_duration_hours").between(0.5, 2), 1).otherwise(0)).alias("normal_pickups"),
                            sum(when(col("pickup_duration_hours").gt(2), 1).otherwise(0)).alias("slow_pickups")
                    )
                    .withColumn("fast_pickup_rate", col("fast_pickups").divide(col("total_pickups")))
                    .withColumn("slow_pickup_rate", col("slow_pickups").divide(col("total_pickups")));

            // ä¿å­˜åˆ°HDFS
            deliveryTimeMetrics.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/delivery_time_metrics");

            pickupTimeMetrics.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/pickup_time_metrics");

            System.out.println("âœ… æ—¶é—´æ•ˆç‡åˆ†æå®Œæˆ");

        } catch (Exception e) {
            System.err.println("âŒ æ—¶é—´æ•ˆç‡åˆ†æå¤±è´¥: " + e.getMessage());
            throw e;
        }
    }

    /**
     * 2. ç©ºé—´åœ°ç†åˆ†æ
     */
    private static void generateSpatialAnalysisMetrics(Dataset<Row> delivery, Dataset<Row> pickup, String outputPath, SparkSession spark) {
        try {
            // é…é€ç©ºé—´åˆ†æ - ç½‘æ ¼åŒ–çƒ­åŠ›å›¾
            Dataset<Row> deliverySpatialMetrics = delivery
                    .withColumn("lng_grid", floor(col("lng").multiply(100)).divide(100)) // 0.01åº¦ç½‘æ ¼
                    .withColumn("lat_grid", floor(col("lat").multiply(100)).divide(100))
                    .groupBy("city", "date", "lng_grid", "lat_grid")
                    .agg(
                            count("order_id").alias("delivery_count"),
                            countDistinct("courier_id").alias("unique_couriers"),
                            avg("delivery_duration_hours").alias("avg_delivery_time"),
                            avg("delivery_distance_km").alias("avg_delivery_distance")
                    )
                    .withColumn("delivery_density", col("delivery_count").divide(0.01 * 0.01)); // æ¯å¹³æ–¹åº¦çš„é…é€å¯†åº¦

            // åŒºåŸŸè¦†ç›–åˆ†æ
            Dataset<Row> regionCoverageMetrics = delivery
                    .groupBy("city", "date", "aoi_id")
                    .agg(
                            count("order_id").alias("orders_in_aoi"),
                            countDistinct("courier_id").alias("couriers_in_aoi"),
                            avg("delivery_duration_hours").alias("avg_aoi_delivery_time"),
                            first("aoi_name").alias("aoi_name")
                    )
                    .withColumn("orders_per_courier", col("orders_in_aoi").divide(col("couriers_in_aoi")));

            // ä¿å­˜åˆ°HDFS
            deliverySpatialMetrics.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/delivery_spatial_metrics");

            regionCoverageMetrics.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/region_coverage_metrics");

            System.out.println("âœ… ç©ºé—´åœ°ç†åˆ†æå®Œæˆ");

        } catch (Exception e) {
            System.err.println("âŒ ç©ºé—´åœ°ç†åˆ†æå¤±è´¥: " + e.getMessage());
            throw e;
        }
    }

    /**
     * 3. è¿è¥æ•ˆç‡åˆ†æ
     */
    private static void generateOperationalEfficiencyMetrics(Dataset<Row> delivery, Dataset<Row> pickup, String outputPath, SparkSession spark) {
        try {
            // å¿«é€’å‘˜æ•ˆç‡åˆ†æ
            Dataset<Row> courierEfficiencyMetrics = delivery
                    .groupBy("city", "region_id", "courier_id", "date", "hour")
                    .agg(
                            count("order_id").alias("total_orders"),
                            countDistinct("aoi_id").alias("unique_aoi_served"),
                            sum("delivery_distance_km").alias("total_distance"),
                            sum("delivery_duration_hours").alias("total_working_hours")
                    )
                    .withColumn("orders_per_hour", col("total_orders").divide(col("total_working_hours")))
                    .withColumn("distance_per_order", col("total_distance").divide(col("total_orders")))
                    .withColumn("working_hours", col("total_working_hours"))
                    .filter(col("total_working_hours").gt(0));

            // åŒºåŸŸè´Ÿè½½åˆ†æ
            Dataset<Row> regionLoadMetrics = delivery
                    .groupBy("city", "region_id", "date", "hour")
                    .agg(
                            count("order_id").alias("total_region_orders"),
                            countDistinct("courier_id").alias("active_couriers"),
                            countDistinct("aoi_id").alias("served_aois"),
                            avg("delivery_duration_hours").alias("avg_region_delivery_time")
                    )
                    .withColumn("orders_per_courier", col("total_region_orders").divide(col("active_couriers")))
                    .withColumn("orders_per_aoi", col("total_region_orders").divide(col("served_aois")))
                    .withColumn("region_load_score",
                            col("orders_per_courier").multiply(0.6).plus(col("orders_per_aoi").multiply(0.4)));

            // ä¿å­˜åˆ°HDFS
            courierEfficiencyMetrics.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/courier_efficiency_metrics");

            regionLoadMetrics.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/region_load_metrics");

            System.out.println("âœ… è¿è¥æ•ˆç‡åˆ†æå®Œæˆ");

        } catch (Exception e) {
            System.err.println("âŒ è¿è¥æ•ˆç‡åˆ†æå¤±è´¥: " + e.getMessage());
            throw e;
        }
    }

    /**
     * 4. é¢„æµ‹åˆ†ææ•°æ®ç”Ÿæˆ
     */
    private static void generatePredictiveAnalysisData(Dataset<Row> delivery, Dataset<Row> pickup, String outputPath, SparkSession spark) {
        try {
            // æ—¶é—´åºåˆ—è¶‹åŠ¿æ•°æ®
            Dataset<Row> timeSeriesTrends = delivery
                    .groupBy("city", "date", "hour")
                    .agg(
                            count("order_id").alias("order_volume"),
                            countDistinct("courier_id").alias("courier_count"),
                            avg("delivery_duration_hours").alias("avg_duration"),
                            sum("delivery_distance_km").alias("total_distance")
                    )
                    .withColumn("volume_trend", lag("order_volume", 1).over(
                            Window.partitionBy("city").orderBy(
                                    unix_timestamp(col("date")).plus(col("hour").multiply(3600))
                            )))
                    .withColumn("efficiency_score",
                            col("order_volume").divide(col("courier_count").multiply(col("avg_duration"))));

            // å®¹é‡è§„åˆ’æ•°æ®
            Dataset<Row> capacityPlanningData = delivery
                    .groupBy("city", "region_id", "date")
                    .agg(
                            count("order_id").alias("daily_orders"),
                            countDistinct("courier_id").alias("required_couriers"),
                            max("hour").alias("peak_hour"),
                            sum("delivery_distance_km").alias("total_daily_distance")
                    )
                    .withColumn("orders_per_courier_day", col("daily_orders").divide(col("required_couriers")))
                    .withColumn("capacity_utilization",
                            when(col("orders_per_courier_day").gt(50), lit("HIGH"))
                                    .when(col("orders_per_courier_day").gt(30), lit("MEDIUM"))
                                    .otherwise(lit("LOW")));

            // ä¿å­˜åˆ°HDFS
            timeSeriesTrends.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/time_series_trends");

            capacityPlanningData.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/capacity_planning_data");

            System.out.println("âœ… é¢„æµ‹åˆ†ææ•°æ®ç”Ÿæˆå®Œæˆ");

        } catch (Exception e) {
            System.err.println("âŒ é¢„æµ‹åˆ†ææ•°æ®ç”Ÿæˆå¤±è´¥: " + e.getMessage());
            throw e;
        }
    }

    /**
     * 5. æˆæœ¬æ•ˆç›Šåˆ†æ
     */
    private static void generateCostAnalysisMetrics(Dataset<Row> delivery, Dataset<Row> pickup, String outputPath, SparkSession spark) {
        try {
            // æˆæœ¬ç»“æ„åˆ†æ
            Dataset<Row> costStructureMetrics = delivery
                    .withColumn("fuel_cost", col("delivery_distance_km").multiply(0.5)) // å‡è®¾æ¯å…¬é‡Œ0.5å…ƒæ²¹è´¹
                    .withColumn("time_cost", col("delivery_duration_hours").multiply(20)) // å‡è®¾æ¯å°æ—¶20å…ƒäººå·¥æˆæœ¬
                    .withColumn("total_delivery_cost", col("fuel_cost").plus(col("time_cost")))
                    .groupBy("city", "region_id", "date")
                    .agg(
                            sum("total_delivery_cost").alias("total_cost"),
                            sum("fuel_cost").alias("total_fuel_cost"),
                            sum("time_cost").alias("total_time_cost"),
                            count("order_id").alias("total_orders"),
                            sum("delivery_distance_km").alias("total_distance")
                    )
                    .withColumn("cost_per_order", col("total_cost").divide(col("total_orders")))
                    .withColumn("cost_per_km", col("total_cost").divide(col("total_distance")))
                    .withColumn("fuel_cost_ratio", col("total_fuel_cost").divide(col("total_cost")));

            // æ•ˆç›Šè¯„ä¼°
            Dataset<Row> efficiencyROI = delivery
                    .groupBy("city", "courier_id", "date")
                    .agg(
                            count("order_id").alias("completed_orders"),
                            sum("delivery_distance_km").alias("distance_covered"),
                            sum("delivery_duration_hours").alias("working_hours")
                    )
                    .withColumn("productivity_score",
                            col("completed_orders").divide(col("working_hours")))
                    .withColumn("efficiency_rating",
                            when(col("productivity_score").gt(10), lit("EXCELLENT"))
                                    .when(col("productivity_score").gt(7), lit("GOOD"))
                                    .when(col("productivity_score").gt(5), lit("AVERAGE"))
                                    .otherwise(lit("NEEDS_IMPROVEMENT")));

            // ä¿å­˜åˆ°HDFS
            costStructureMetrics.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/cost_structure_metrics");

            efficiencyROI.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/efficiency_roi_metrics");

            System.out.println("âœ… æˆæœ¬æ•ˆç›Šåˆ†æå®Œæˆ");

        } catch (Exception e) {
            System.err.println("âŒ æˆæœ¬æ•ˆç›Šåˆ†æå¤±è´¥: " + e.getMessage());
            throw e;
        }
    }

    /**
     * 6. KPIç›‘æ§æŒ‡æ ‡ç”Ÿæˆ
     */
    private static void generateKPIMetrics(Dataset<Row> delivery, Dataset<Row> pickup, String outputPath, SparkSession spark) {
        try {
            // æ ¸å¿ƒKPIæŒ‡æ ‡
            Dataset<Row> coreKPIs = delivery
                    .groupBy("city", "date", "hour")
                    .agg(
                            count("order_id").alias("total_orders"),
                            countDistinct("courier_id").alias("active_couriers"),
                            countDistinct("aoi_id").alias("coverage_aois"),
                            avg("delivery_duration_hours").alias("avg_delivery_time"),
                            sum(when(col("delivery_duration_hours").leq(1), 1).otherwise(0)).alias("on_time_deliveries")
                    )
                    .withColumn("orders_per_courier", col("total_orders").divide(col("active_couriers")))
                    .withColumn("orders_per_aoi", col("total_orders").divide(col("coverage_aois")))
                    .withColumn("on_time_rate", col("on_time_deliveries").divide(col("total_orders")))
                    .withColumn("efficiency_score",
                            col("orders_per_courier").multiply(0.4)
                                    .plus(col("on_time_rate").multiply(0.6)))
                    .withColumn("kpi_timestamp", current_timestamp());

            // æœåŠ¡è´¨é‡KPI
            Dataset<Row> serviceQualityKPIs = delivery
                    .groupBy("city", "date", "hour")
                    .agg(
                            count("order_id").alias("total_deliveries"),
                            sum(when(col("delivery_duration_hours").leq(1), 1).otherwise(0)).alias("fast_deliveries"),
                            sum(when(col("delivery_duration_hours").between(1, 4), 1).otherwise(0)).alias("normal_deliveries"),
                            sum(when(col("delivery_duration_hours").gt(4), 1).otherwise(0)).alias("slow_deliveries"),
                            avg("delivery_duration_hours").alias("avg_delivery_duration")
                    )
                    .withColumn("fast_delivery_rate", col("fast_deliveries").divide(col("total_deliveries")))
                    .withColumn("slow_delivery_rate", col("slow_deliveries").divide(col("total_deliveries")))
                    .withColumn("service_score",
                            col("fast_delivery_rate").multiply(100).minus(col("slow_delivery_rate").multiply(50)));

            // ä¿å­˜åˆ°HDFS
            coreKPIs.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/core_kpis");

            serviceQualityKPIs.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/service_quality_kpis");

            // å†™å…¥MySQLå®æ—¶KPIè¡¨
            writeKPIsToMySQL(coreKPIs, serviceQualityKPIs, spark);

            System.out.println("âœ… KPIç›‘æ§æŒ‡æ ‡ç”Ÿæˆå®Œæˆ");

        } catch (Exception e) {
            System.err.println("âŒ KPIç›‘æ§æŒ‡æ ‡ç”Ÿæˆå¤±è´¥: " + e.getMessage());
            throw e;
        }
    }

    /**
     * 7. å¼‚å¸¸æ£€æµ‹åˆ†æ
     */
    private static void generateAnomalyDetectionMetrics(Dataset<Row> delivery, Dataset<Row> pickup, String outputPath, SparkSession spark) {
        try {
            // æ—¶é—´å¼‚å¸¸æ£€æµ‹
            Dataset<Row> timeAnomalies = delivery
                    .withColumn("delivery_time_zscore",
                            (col("delivery_duration_hours").minus(lit(2.5))).divide(lit(1.5))) // å‡è®¾å‡å€¼2.5å°æ—¶ï¼Œæ ‡å‡†å·®1.5å°æ—¶
                    .filter(abs(col("delivery_time_zscore")).gt(2)) // Z-score > 2 è§†ä¸ºå¼‚å¸¸
                    .select("order_id", "city", "courier_id", "delivery_duration_hours", "delivery_time_zscore", "date")
                    .withColumn("anomaly_type", lit("TIME_ANOMALY"))
                    .withColumn("anomaly_severity",
                            when(abs(col("delivery_time_zscore")).gt(3), lit("HIGH"))
                                    .when(abs(col("delivery_time_zscore")).gt(2.5), lit("MEDIUM"))
                                    .otherwise(lit("LOW")));

            // è·ç¦»å¼‚å¸¸æ£€æµ‹
            Dataset<Row> distanceAnomalies = delivery
                    .withColumn("distance_zscore",
                            (col("delivery_distance_km").minus(lit(15))).divide(lit(10))) // å‡è®¾å‡å€¼15kmï¼Œæ ‡å‡†å·®10km
                    .filter(abs(col("distance_zscore")).gt(2))
                    .select("order_id", "city", "courier_id", "delivery_distance_km", "distance_zscore", "date")
                    .withColumn("anomaly_type", lit("DISTANCE_ANOMALY"))
                    .withColumn("anomaly_severity",
                            when(abs(col("distance_zscore")).gt(3), lit("HIGH"))
                                    .when(abs(col("distance_zscore")).gt(2.5), lit("MEDIUM"))
                                    .otherwise(lit("LOW")));

            // ä¿å­˜åˆ°HDFS
            timeAnomalies.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/time_anomalies");

            distanceAnomalies.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/distance_anomalies");

            // å†™å…¥MySQLå¼‚å¸¸å‘Šè­¦è¡¨
            writeAnomaliesToMySQL(timeAnomalies, distanceAnomalies, spark);

            System.out.println("âœ… å¼‚å¸¸æ£€æµ‹åˆ†æå®Œæˆ");

        } catch (Exception e) {
            System.err.println("âŒ å¼‚å¸¸æ£€æµ‹åˆ†æå¤±è´¥: " + e.getMessage());
            throw e;
        }
    }

    /**
     * 8. ç»¼åˆæŠ¥è¡¨æ•°æ®ç”Ÿæˆ
     */
    private static void generateComprehensiveReports(Dataset<Row> delivery, Dataset<Row> pickup, String outputPath, SparkSession spark) {
        try {
            // æ—¥æŠ¥æ•°æ®
            Dataset<Row> dailyReports = delivery
                    .groupBy("city", "region_id", "date")
                    .agg(
                            count("order_id").alias("total_orders"),
                            countDistinct("courier_id").alias("active_couriers"),
                            countDistinct("aoi_id").alias("served_aois"),
                            avg("delivery_duration_hours").alias("avg_delivery_time"),
                            sum("delivery_distance_km").alias("total_distance"),
                            sum(when(col("delivery_duration_hours").leq(1), 1).otherwise(0)).alias("fast_deliveries")
                    )
                    .withColumn("avg_orders_per_courier", col("total_orders").divide(col("active_couriers")))
                    .withColumn("avg_distance_per_order", col("total_distance").divide(col("total_orders")))
                    .withColumn("fast_delivery_rate", col("fast_deliveries").divide(col("total_orders")))
                    .withColumn("report_type", lit("DAILY"))
                    .withColumn("generated_at", current_timestamp());

            // å‘¨æŠ¥æ•°æ® (æŒ‰å‘¨èšåˆ)
            Dataset<Row> weeklyReports = delivery
                    .withColumn("week_start", date_trunc("week", col("date")))
                    .groupBy("city", "region_id", "week_start")
                    .agg(
                            count("order_id").alias("weekly_total_orders"),
                            countDistinct("courier_id").alias("weekly_active_couriers"),
                            avg("delivery_duration_hours").alias("weekly_avg_delivery_time"),
                            sum("delivery_distance_km").alias("weekly_total_distance")
                    )
                    .withColumn("report_type", lit("WEEKLY"))
                    .withColumn("generated_at", current_timestamp());

            // ä¿å­˜åˆ°HDFS
            dailyReports.write()
                    .mode("overwrite")
                    .partitionBy("city", "date")
                    .parquet(outputPath + "/daily_reports");

            weeklyReports.write()
                    .mode("overwrite")
                    .partitionBy("city")
                    .parquet(outputPath + "/weekly_reports");

            System.out.println("âœ… ç»¼åˆæŠ¥è¡¨æ•°æ®ç”Ÿæˆå®Œæˆ");

        } catch (Exception e) {
            System.err.println("âŒ ç»¼åˆæŠ¥è¡¨æ•°æ®ç”Ÿæˆå¤±è´¥: " + e.getMessage());
            throw e;
        }
    }

    /**
     * å†™å…¥KPIæ•°æ®åˆ°MySQL
     */
    private static void writeKPIsToMySQL(Dataset<Row> coreKPIs, Dataset<Row> serviceQualityKPIs, SparkSession spark) {
        try {
            Properties mysqlProps = new Properties();
            mysqlProps.setProperty("user", "root");
            mysqlProps.setProperty("password", "password");
            mysqlProps.setProperty("driver", "com.mysql.cj.jdbc.Driver");

            String mysqlUrl = "jdbc:mysql://localhost:3306/logistics_db";

            // åªå†™å…¥æœ€è¿‘7å¤©çš„KPIæ•°æ®åˆ°MySQL
            Dataset<Row> recentKPIs = coreKPIs
                    .filter(col("date").geq(date_sub(current_date(), 7)))
                    .select("city", "date", "hour", "total_orders", "active_couriers",
                            "coverage_aois", "orders_per_courier", "orders_per_aoi", "efficiency_score");

            recentKPIs.write()
                    .mode("overwrite")
                    .jdbc(mysqlUrl, "realtime_kpi", mysqlProps);

            System.out.println("âœ… KPIæ•°æ®å·²å†™å…¥MySQL");

        } catch (Exception e) {
            System.err.println("âš ï¸ MySQLå†™å…¥å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ: " + e.getMessage());
        }
    }

    /**
     * å†™å…¥å¼‚å¸¸æ•°æ®åˆ°MySQL
     */
    private static void writeAnomaliesToMySQL(Dataset<Row> timeAnomalies, Dataset<Row> distanceAnomalies, SparkSession spark) {
        try {
            Properties mysqlProps = new Properties();
            mysqlProps.setProperty("user", "root");
            mysqlProps.setProperty("password", "password");
            mysqlProps.setProperty("driver", "com.mysql.cj.jdbc.Driver");

            String mysqlUrl = "jdbc:mysql://localhost:3306/logistics_db";

            // å¤„ç†æ—¶é—´å¼‚å¸¸
            Dataset<Row> timeAlerts = timeAnomalies
                    .selectExpr(
                            "'TIME_ANOMALY' as alert_type",
                            "city",
                            "order_id",
                            "courier_id",
                            "anomaly_severity as severity",
                            "concat('é…é€æ—¶é—´å¼‚å¸¸: ', delivery_duration_hours, ' å°æ—¶') as description",
                            "delivery_duration_hours as anomaly_value",
                            "2.5 as threshold_value",
                            "false as is_resolved"
                    );

            // å¤„ç†è·ç¦»å¼‚å¸¸
            Dataset<Row> distanceAlerts = distanceAnomalies
                    .selectExpr(
                            "'DISTANCE_ANOMALY' as alert_type",
                            "city",
                            "order_id",
                            "courier_id",
                            "anomaly_severity as severity",
                            "concat('é…é€è·ç¦»å¼‚å¸¸: ', delivery_distance_km, ' å…¬é‡Œ') as description",
                            "delivery_distance_km as anomaly_value",
                            "15.0 as threshold_value",
                            "false as is_resolved"
                    );

            // åˆå¹¶å†™å…¥
            Dataset<Row> allAlerts = timeAlerts.union(distanceAlerts);

            allAlerts.write()
                    .mode("append")
                    .jdbc(mysqlUrl, "anomaly_alerts", mysqlProps);

            System.out.println("âœ… å¼‚å¸¸å‘Šè­¦æ•°æ®å·²å†™å…¥MySQL");

        } catch (Exception e) {
            System.err.println("âš ï¸ å¼‚å¸¸æ•°æ®MySQLå†™å…¥å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ: " + e.getMessage());
        }
    }
}