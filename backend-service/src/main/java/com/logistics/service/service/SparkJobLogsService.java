package com.logistics.service.service;

import com.logistics.service.dao.entity.SparkJobLogs;
import com.logistics.service.dao.mapper.SparkJobLogsMapper;
import com.logistics.service.dto.SparkJobLogsDTO;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.cache.annotation.*;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.time.LocalDateTime;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

@Slf4j
@Service
@Transactional(readOnly = true)
public class SparkJobLogsService {

    @Autowired
    private SparkJobLogsMapper sparkJobLogsMapper;

    // ==================== æ•°æ®ä¿å­˜æ“ä½œ ====================

    /**
     * ä¿å­˜ä½œä¸šæ—¥å¿— - ä¿å­˜åæ¸…é™¤ç¼“å­˜
     */
    @Transactional
    @CacheEvict(value = {"jobs", "stats"}, allEntries = true)
    public int saveJob(SparkJobLogs job) {
        validateJob(job);

        int result = sparkJobLogsMapper.insertJob(job);
        if (result > 0) {
            log.info("âœ… ä¿å­˜ä½œä¸šæ—¥å¿—æˆåŠŸï¼Œä½œä¸š: {}ï¼Œå·²æ¸…é™¤ç¼“å­˜", job.getJobName());
        }
        return result;
    }

    /**
     * æ‰¹é‡ä¿å­˜ä½œä¸šæ—¥å¿— - ä¿å­˜åæ¸…é™¤ç¼“å­˜
     */
    @Transactional
    @CacheEvict(value = {"jobs", "stats"}, allEntries = true)
    public int batchSaveJobs(List<SparkJobLogs> jobList) {
        for (SparkJobLogs job : jobList) {
            validateJob(job);
        }

        int result = sparkJobLogsMapper.batchInsertJobs(jobList);
        if (result > 0) {
            log.info("âœ… æ‰¹é‡ä¿å­˜ä½œä¸šæ—¥å¿—æˆåŠŸï¼Œå…±ä¿å­˜ {} æ¡ï¼Œå·²æ¸…é™¤ç¼“å­˜", result);
        }
        return result;
    }

    /**
     * æ›´æ–°ä½œä¸šçŠ¶æ€ - æ›´æ–°åæ¸…é™¤ç¼“å­˜
     */
    @Transactional
    @CacheEvict(value = {"jobs", "stats"}, allEntries = true)
    public int updateJobStatus(Long id, String status, LocalDateTime endTime,
                               Integer executionTimeSeconds, String errorMessage) {
        int result = sparkJobLogsMapper.updateJobStatus(id, status, endTime, executionTimeSeconds, errorMessage);
        if (result > 0) {
            log.info("âœ… æ›´æ–°ä½œä¸šçŠ¶æ€æˆåŠŸï¼ŒID: {}ï¼Œå·²æ¸…é™¤ç¼“å­˜", id);
        }
        return result;
    }

    // ==================== æŸ¥è¯¢æ“ä½œ ====================

    /**
     * è·å–æœ€è¿‘çš„ä½œä¸šæ—¥å¿— - æ·»åŠ ç¼“å­˜
     */
    @Cacheable(value = "jobs", key = "'recent:' + #limit", unless = "#result.isEmpty()")
    public List<SparkJobLogsDTO> getRecentJobLogs(int limit) {
        try {
            log.info("ğŸ” æŸ¥è¯¢æ•°æ®åº“è·å–æœ€è¿‘ä½œä¸šæ—¥å¿—[limit={}]", limit);
            List<SparkJobLogs> logs = sparkJobLogsMapper.findRecentJobs(limit);
            return logs.stream().map(this::convertToDTO).collect(Collectors.toList());
        } catch (Exception e) {
            log.error("è·å–æœ€è¿‘ä½œä¸šæ—¥å¿—å¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * æŒ‰çŠ¶æ€è·å–ä½œä¸šæ—¥å¿— - æ·»åŠ ç¼“å­˜
     */
    @Cacheable(value = "jobs", key = "'status:' + #status", unless = "#result.isEmpty()")
    public List<SparkJobLogsDTO> getJobLogsByStatus(String status) {
        try {
            log.info("ğŸ” æŸ¥è¯¢æ•°æ®åº“è·å–çŠ¶æ€ä½œä¸šæ—¥å¿—[status={}]", status);
            List<SparkJobLogs> logs = sparkJobLogsMapper.findByStatus(status);
            return logs.stream().map(this::convertToDTO).collect(Collectors.toList());
        } catch (Exception e) {
            log.error("æŒ‰çŠ¶æ€è·å–ä½œä¸šæ—¥å¿—å¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * æŒ‰ä½œä¸šåç§°è·å–æ—¥å¿— - æ·»åŠ ç¼“å­˜
     */
    @Cacheable(value = "jobs", key = "'name:' + #jobName", unless = "#result.isEmpty()")
    public List<SparkJobLogsDTO> getJobLogsByName(String jobName) {
        try {
            log.info("ğŸ” æŸ¥è¯¢æ•°æ®åº“è·å–ä½œä¸šæ—¥å¿—[jobName={}]", jobName);
            List<SparkJobLogs> logs = sparkJobLogsMapper.findByJobName(jobName);
            return logs.stream().map(this::convertToDTO).collect(Collectors.toList());
        } catch (Exception e) {
            log.error("æŒ‰ä½œä¸šåç§°è·å–æ—¥å¿—å¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * è·å–å¤±è´¥çš„ä½œä¸š - æ·»åŠ ç¼“å­˜
     */
    @Cacheable(value = "jobs", key = "'failed'", unless = "#result.isEmpty()")
    public List<SparkJobLogsDTO> getFailedJobs() {
        return getJobLogsByStatus("FAILED");
    }

    /**
     * è·å–è¿è¡Œä¸­çš„ä½œä¸š - æ·»åŠ ç¼“å­˜
     */
    @Cacheable(value = "jobs", key = "'running'", unless = "#result.isEmpty()")
    public List<SparkJobLogsDTO> getRunningJobs() {
        return getJobLogsByStatus("RUNNING");
    }

    /**
     * è·å–æˆåŠŸçš„ä½œä¸š - æ·»åŠ ç¼“å­˜
     */
    @Cacheable(value = "jobs", key = "'successful:' + #limit", unless = "#result.isEmpty()")
    public List<SparkJobLogsDTO> getSuccessfulJobs(int limit) {
        try {
            log.info("ğŸ” æŸ¥è¯¢æ•°æ®åº“è·å–æˆåŠŸä½œä¸š[limit={}]", limit);
            List<SparkJobLogs> logs = sparkJobLogsMapper.findSuccessfulJobs(limit);
            return logs.stream().map(this::convertToDTO).collect(Collectors.toList());
        } catch (Exception e) {
            log.error("è·å–æˆåŠŸä½œä¸šå¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * è·å–æŒ‡å®šæ—¶é—´èŒƒå›´çš„ä½œä¸šæ—¥å¿— - æ·»åŠ ç¼“å­˜
     */
    @Cacheable(value = "jobs", key = "'range:' + #startTime + ':' + #endTime", unless = "#result.isEmpty()")
    public List<SparkJobLogsDTO> getJobLogsByTimeRange(LocalDateTime startTime, LocalDateTime endTime) {
        try {
            log.info("ğŸ” æŸ¥è¯¢æ•°æ®åº“è·å–æ—¶é—´èŒƒå›´ä½œä¸šæ—¥å¿—[{} - {}]", startTime, endTime);
            List<SparkJobLogs> logs = sparkJobLogsMapper.findByTimeRange(startTime, endTime);
            return logs.stream().map(this::convertToDTO).collect(Collectors.toList());
        } catch (Exception e) {
            log.error("æŒ‰æ—¶é—´èŒƒå›´è·å–ä½œä¸šæ—¥å¿—å¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * è·å–ä»Šæ—¥ä½œä¸šæ—¥å¿— - æ·»åŠ ç¼“å­˜
     */
    @Cacheable(value = "jobs", key = "'today'", unless = "#result.isEmpty()")
    public List<SparkJobLogsDTO> getTodayJobs() {
        LocalDateTime startOfDay = LocalDateTime.now().withHour(0).withMinute(0).withSecond(0).withNano(0);
        LocalDateTime endOfDay = startOfDay.plusDays(1).minusNanos(1);
        return getJobLogsByTimeRange(startOfDay, endOfDay);
    }

    /**
     * è·å–é•¿æ—¶é—´è¿è¡Œçš„ä½œä¸š - æ·»åŠ ç¼“å­˜
     */
    @Cacheable(value = "jobs", key = "'long_running:' + #thresholdMinutes", unless = "#result.isEmpty()")
    public List<SparkJobLogsDTO> getLongRunningJobs(int thresholdMinutes) {
        try {
            log.info("ğŸ” æŸ¥è¯¢æ•°æ®åº“è·å–é•¿æ—¶é—´è¿è¡Œä½œä¸š[threshold={}åˆ†é’Ÿ]", thresholdMinutes);
            List<SparkJobLogs> logs = sparkJobLogsMapper.findLongRunningJobs(thresholdMinutes * 60);
            return logs.stream().map(this::convertToDTO).collect(Collectors.toList());
        } catch (Exception e) {
            log.error("è·å–é•¿æ—¶é—´è¿è¡Œä½œä¸šå¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * è·å–æœ‰é”™è¯¯çš„ä½œä¸š - æ·»åŠ ç¼“å­˜
     */
    @Cacheable(value = "jobs", key = "'with_errors:' + #limit", unless = "#result.isEmpty()")
    public List<SparkJobLogsDTO> getJobsWithErrors(int limit) {
        try {
            log.info("ğŸ” æŸ¥è¯¢æ•°æ®åº“è·å–æœ‰é”™è¯¯çš„ä½œä¸š[limit={}]", limit);
            List<SparkJobLogs> logs = sparkJobLogsMapper.findJobsWithErrors(limit);
            return logs.stream().map(this::convertToDTO).collect(Collectors.toList());
        } catch (Exception e) {
            log.error("è·å–æœ‰é”™è¯¯çš„ä½œä¸šå¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * æ ¹æ®IDè·å–ä½œä¸šè¯¦æƒ… - æ·»åŠ ç¼“å­˜
     */
    @Cacheable(value = "jobs", key = "'id:' + #id", unless = "#result == null")
    public SparkJobLogsDTO getJobById(Long id) {
        try {
            log.info("ğŸ” æŸ¥è¯¢æ•°æ®åº“è·å–ä½œä¸šè¯¦æƒ…[id={}]", id);
            SparkJobLogs job = sparkJobLogsMapper.findById(id);
            return job != null ? convertToDTO(job) : null;
        } catch (Exception e) {
            log.error("æ ¹æ®IDè·å–ä½œä¸šå¤±è´¥", e);
            return null;
        }
    }

    // ==================== ç»Ÿè®¡åˆ†ææ“ä½œ ====================

    /**
     * è·å–ä½œä¸šç»Ÿè®¡ä¿¡æ¯ - æ·»åŠ ç¼“å­˜
     */
    @Cacheable(value = "stats", key = "'statistics'")
    public Map<String, Object> getJobStatistics() {
        try {
            log.info("ğŸ” æŸ¥è¯¢æ•°æ®åº“è·å–ä½œä¸šç»Ÿè®¡ä¿¡æ¯");
            Map<String, Object> statistics = new HashMap<>();

            // è·å–æ€»æ•°é‡
            int totalJobs = sparkJobLogsMapper.countTotalJobs();
            int successJobs = sparkJobLogsMapper.countJobsByStatus("SUCCESS");
            int failedJobs = sparkJobLogsMapper.countJobsByStatus("FAILED");
            int runningJobs = sparkJobLogsMapper.countJobsByStatus("RUNNING");

            // è®¡ç®—æ¯”ç‡
            double successRate = totalJobs > 0 ? (double) successJobs / totalJobs * 100 : 0;
            double failureRate = totalJobs > 0 ? (double) failedJobs / totalJobs * 100 : 0;

            // è·å–å¹³å‡æ‰§è¡Œæ—¶é—´å’Œæ€»å¤„ç†è®°å½•æ•°
            Double avgExecutionTime = sparkJobLogsMapper.getAverageExecutionTime();
            Long totalProcessedRecords = sparkJobLogsMapper.getTotalProcessedRecords();

            statistics.put("totalJobs", totalJobs);
            statistics.put("successJobs", successJobs);
            statistics.put("failedJobs", failedJobs);
            statistics.put("runningJobs", runningJobs);
            statistics.put("successRate", Math.round(successRate * 100.0) / 100.0);
            statistics.put("failureRate", Math.round(failureRate * 100.0) / 100.0);
            statistics.put("avgExecutionTime", avgExecutionTime != null ? Math.round(avgExecutionTime) : 0);
            statistics.put("totalProcessedRecords", totalProcessedRecords != null ? totalProcessedRecords : 0L);

            return statistics;
        } catch (Exception e) {
            log.error("è·å–ä½œä¸šç»Ÿè®¡å¤±è´¥", e);
            Map<String, Object> errorStats = new HashMap<>();
            errorStats.put("error", "ç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥");
            return errorStats;
        }
    }

    /**
     * è·å–ä½œä¸šæ‰§è¡Œè¶‹åŠ¿ - æ·»åŠ ç¼“å­˜
     */
    @Cacheable(value = "stats", key = "'trend:' + #days", unless = "#result.isEmpty()")
    public List<Map<String, Object>> getJobExecutionTrend(int days) {
        try {
            log.info("ğŸ” æŸ¥è¯¢æ•°æ®åº“è·å–ä½œä¸šæ‰§è¡Œè¶‹åŠ¿[days={}]", days);
            LocalDateTime startTime = LocalDateTime.now().minusDays(days);
            return sparkJobLogsMapper.getJobExecutionTrend(startTime);
        } catch (Exception e) {
            log.error("è·å–ä½œä¸šæ‰§è¡Œè¶‹åŠ¿å¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    // ==================== æ•°æ®ç»´æŠ¤ ====================

    /**
     * æ¸…ç†æ—§çš„ä½œä¸šæ—¥å¿— - æ¸…ç†åæ¸…é™¤ç¼“å­˜
     */
    @Transactional
    @CacheEvict(value = {"jobs", "stats"}, allEntries = true)
    public int cleanupOldJobs(int daysToKeep) {
        try {
            LocalDateTime cutoffTime = LocalDateTime.now().minusDays(daysToKeep);
            int deletedCount = sparkJobLogsMapper.cleanupOldJobs(cutoffTime);
            if (deletedCount > 0) {
                log.info("âœ… æ¸…ç†æ—§ä½œä¸šæ—¥å¿—å®Œæˆï¼Œåˆ é™¤ {} æ¡è®°å½•ï¼Œå·²æ¸…é™¤ç¼“å­˜", deletedCount);
            }
            return deletedCount;
        } catch (Exception e) {
            log.error("æ¸…ç†æ—§ä½œä¸šæ—¥å¿—å¤±è´¥", e);
            return 0;
        }
    }

    // ==================== ç§æœ‰æ–¹æ³• ====================

    /**
     * æ•°æ®éªŒè¯
     */
    private void validateJob(SparkJobLogs job) {
        if (job.getJobName() == null || job.getJobName().trim().isEmpty()) {
            throw new IllegalArgumentException("ä½œä¸šåç§°ä¸èƒ½ä¸ºç©º");
        }
        if (job.getStatus() == null || job.getStatus().trim().isEmpty()) {
            throw new IllegalArgumentException("ä½œä¸šçŠ¶æ€ä¸èƒ½ä¸ºç©º");
        }
    }

    /**
     * å®ä½“è½¬DTO
     */
    private SparkJobLogsDTO convertToDTO(SparkJobLogs logs) {
        SparkJobLogsDTO dto = new SparkJobLogsDTO();
        dto.setId(logs.getId());
        dto.setJobName(logs.getJobName());
        dto.setStartTime(logs.getStartTime());
        dto.setEndTime(logs.getEndTime());
        dto.setStatus(logs.getStatus());
        dto.setInputDeliverPath(logs.getInputDeliverPath());
        dto.setInputPickupPath(logs.getInputPickupPath());
        dto.setOutputPath(logs.getOutputPath());
        dto.setProcessedRecords(logs.getProcessedRecords());
        dto.setErrorMessage(logs.getErrorMessage());
        dto.setExecutionTimeSeconds(logs.getExecutionTimeSeconds());
        dto.setTimeFormatUsed(logs.getTimeFormatUsed());
        dto.setDefaultYear(logs.getDefaultYear());
        dto.setCreatedAt(logs.getCreatedAt());
        return dto;
    }
}