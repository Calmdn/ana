package com.logistics.service.service;

import com.logistics.service.dao.entity.SparkJobLogs;
import com.logistics.service.dao.mapper.SparkJobLogsMapper;
import com.logistics.service.dto.SparkJobLogsDTO;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.stereotype.Service;
import java.util.concurrent.TimeUnit;
import java.util.stream.Collectors;
import java.time.LocalDateTime;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.HashMap;

@Slf4j
@Service
public class SparkJobLogsService {

    @Autowired
    private SparkJobLogsMapper sparkJobLogsMapper;

    @Autowired
    private RedisTemplate<String, Object> redisTemplate;

    /**
     * è·å–æœ€è¿‘çš„ä½œä¸šæ—¥å¿—
     */
    public List<SparkJobLogsDTO> getRecentJobLogs(int limit) {
        try {
            String key = "spark_jobs:recent:" + limit;

            @SuppressWarnings("unchecked")
            List<SparkJobLogsDTO> cache = (List<SparkJobLogsDTO>) redisTemplate.opsForValue().get(key);
            if (cache != null) {
                log.info("âœ… ä» Redis ç¼“å­˜è·å–æœ€è¿‘ä½œä¸šæ—¥å¿—, size={}", cache.size());
                return cache;
            }

            log.info("ğŸ” Redis æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢ MySQL æœ€è¿‘ä½œä¸šæ—¥å¿—");
            List<SparkJobLogs> logs = sparkJobLogsMapper.findRecentJobs(limit);

            List<SparkJobLogsDTO> result = logs.stream()
                    .map(this::convertToDTO)
                    .collect(Collectors.toList());

            if (!result.isEmpty()) {
                redisTemplate.opsForValue().set(key, result, 5, TimeUnit.MINUTES);
                log.info("ğŸ’¾ å†™å…¥ Redis ç¼“å­˜æœ€è¿‘ä½œä¸šæ—¥å¿—ï¼Œttl=5m");
            }

            return result;
        } catch (Exception e) {
            log.error("è·å–æœ€è¿‘ä½œä¸šæ—¥å¿—å¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * æŒ‰çŠ¶æ€è·å–ä½œä¸šæ—¥å¿—
     */
    public List<SparkJobLogsDTO> getJobLogsByStatus(String status) {
        try {
            String key = "spark_jobs:status:" + status;

            @SuppressWarnings("unchecked")
            List<SparkJobLogsDTO> cache = (List<SparkJobLogsDTO>) redisTemplate.opsForValue().get(key);
            if (cache != null) {
                log.info("âœ… ä» Redis ç¼“å­˜è·å–çŠ¶æ€ä½œä¸šæ—¥å¿—[status={}], size={}", status, cache.size());
                return cache;
            }

            List<SparkJobLogs> logs = sparkJobLogsMapper.findByStatus(status);
            List<SparkJobLogsDTO> result = logs.stream()
                    .map(this::convertToDTO)
                    .collect(Collectors.toList());

            if (!result.isEmpty()) {
                redisTemplate.opsForValue().set(key, result, 3, TimeUnit.MINUTES);
                log.info("ğŸ’¾ å†™å…¥ Redis ç¼“å­˜çŠ¶æ€ä½œä¸šæ—¥å¿—[status={}]ï¼Œttl=3m", status);
            }

            return result;
        } catch (Exception e) {
            log.error("æŒ‰çŠ¶æ€è·å–ä½œä¸šæ—¥å¿—å¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * æŒ‰ä½œä¸šåç§°è·å–æ—¥å¿—
     */
    public List<SparkJobLogsDTO> getJobLogsByName(String jobName) {
        try {
            List<SparkJobLogs> logs = sparkJobLogsMapper.findByJobName(jobName);
            return logs.stream()
                    .map(this::convertToDTO)
                    .collect(Collectors.toList());
        } catch (Exception e) {
            log.error("æŒ‰ä½œä¸šåç§°è·å–æ—¥å¿—å¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * è·å–å¤±è´¥çš„ä½œä¸š
     */
    public List<SparkJobLogsDTO> getFailedJobs() {
        return getJobLogsByStatus("FAILED");
    }

    /**
     * è·å–è¿è¡Œä¸­çš„ä½œä¸š
     */
    public List<SparkJobLogsDTO> getRunningJobs() {
        return getJobLogsByStatus("RUNNING");
    }

    /**
     * è·å–æˆåŠŸçš„ä½œä¸š
     */
    public List<SparkJobLogsDTO> getSuccessfulJobs(int limit) {
        try {
            List<SparkJobLogs> logs = sparkJobLogsMapper.findSuccessfulJobs(limit);
            return logs.stream()
                    .map(this::convertToDTO)
                    .collect(Collectors.toList());
        } catch (Exception e) {
            log.error("è·å–æˆåŠŸä½œä¸šå¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * è·å–æŒ‡å®šæ—¶é—´èŒƒå›´çš„ä½œä¸šæ—¥å¿—
     */
    public List<SparkJobLogsDTO> getJobLogsByTimeRange(LocalDateTime startTime, LocalDateTime endTime) {
        try {
            String key = String.format("spark_jobs:range:%s:%s", startTime, endTime);

            @SuppressWarnings("unchecked")
            List<SparkJobLogsDTO> cache = (List<SparkJobLogsDTO>) redisTemplate.opsForValue().get(key);
            if (cache != null) {
                log.info("âœ… ä» Redis ç¼“å­˜è·å–æ—¶é—´èŒƒå›´ä½œä¸šæ—¥å¿—, size={}", cache.size());
                return cache;
            }

            List<SparkJobLogs> logs = sparkJobLogsMapper.findByTimeRange(startTime, endTime);
            List<SparkJobLogsDTO> result = logs.stream()
                    .map(this::convertToDTO)
                    .collect(Collectors.toList());

            if (!result.isEmpty()) {
                redisTemplate.opsForValue().set(key, result, 15, TimeUnit.MINUTES);
                log.info("ğŸ’¾ å†™å…¥ Redis ç¼“å­˜æ—¶é—´èŒƒå›´ä½œä¸šæ—¥å¿—ï¼Œttl=15m");
            }

            return result;
        } catch (Exception e) {
            log.error("æŒ‰æ—¶é—´èŒƒå›´è·å–ä½œä¸šæ—¥å¿—å¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * è·å–ä»Šæ—¥ä½œä¸šæ—¥å¿—
     */
    public List<SparkJobLogsDTO> getTodayJobs() {
        LocalDateTime startOfDay = LocalDateTime.now().withHour(0).withMinute(0).withSecond(0).withNano(0);
        LocalDateTime endOfDay = startOfDay.plusDays(1).minusNanos(1);
        return getJobLogsByTimeRange(startOfDay, endOfDay);
    }

    /**
     * è·å–é•¿æ—¶é—´è¿è¡Œçš„ä½œä¸š
     */
    public List<SparkJobLogsDTO> getLongRunningJobs(int thresholdMinutes) {
        try {
            List<SparkJobLogs> logs = sparkJobLogsMapper.findLongRunningJobs(thresholdMinutes * 60);
            return logs.stream()
                    .map(this::convertToDTO)
                    .collect(Collectors.toList());
        } catch (Exception e) {
            log.error("è·å–é•¿æ—¶é—´è¿è¡Œä½œä¸šå¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * è·å–ä½œä¸šç»Ÿè®¡ä¿¡æ¯
     */
    public Map<String, Object> getJobStatistics() {
        try {
            String key = "spark_jobs:statistics";

            @SuppressWarnings("unchecked")
            Map<String, Object> cache = (Map<String, Object>) redisTemplate.opsForValue().get(key);
            if (cache != null) {
                log.info("âœ… ä» Redis ç¼“å­˜è·å–ä½œä¸šç»Ÿè®¡ä¿¡æ¯");
                return cache;
            }

            List<SparkJobLogs> allJobs = sparkJobLogsMapper.findRecentJobs(1000);

            long totalJobs = allJobs.size();
            long successJobs = allJobs.stream().filter(job -> "SUCCESS".equals(job.getStatus())).count();
            long failedJobs = allJobs.stream().filter(job -> "FAILED".equals(job.getStatus())).count();
            long runningJobs = allJobs.stream().filter(job -> "RUNNING".equals(job.getStatus())).count();

            double successRate = totalJobs > 0 ? (double) successJobs / totalJobs * 100 : 0;
            double failureRate = totalJobs > 0 ? (double) failedJobs / totalJobs * 100 : 0;

            // è®¡ç®—å¹³å‡æ‰§è¡Œæ—¶é—´
            double avgExecutionTime = allJobs.stream()
                    .filter(job -> job.getExecutionTimeSeconds() != null)
                    .mapToInt(SparkJobLogs::getExecutionTimeSeconds)
                    .average()
                    .orElse(0.0);

            // è®¡ç®—æ€»å¤„ç†è®°å½•æ•°
            long totalProcessedRecords = allJobs.stream()
                    .filter(job -> job.getProcessedRecords() != null)
                    .mapToLong(SparkJobLogs::getProcessedRecords)
                    .sum();

            Map<String, Object> statistics = new HashMap<>();
            statistics.put("totalJobs", totalJobs);
            statistics.put("successJobs", successJobs);
            statistics.put("failedJobs", failedJobs);
            statistics.put("runningJobs", runningJobs);
            statistics.put("successRate", Math.round(successRate * 100.0) / 100.0);
            statistics.put("failureRate", Math.round(failureRate * 100.0) / 100.0);
            statistics.put("avgExecutionTime", Math.round(avgExecutionTime));
            statistics.put("totalProcessedRecords", totalProcessedRecords);

            redisTemplate.opsForValue().set(key, statistics, 10, TimeUnit.MINUTES);
            log.info("ğŸ’¾ å†™å…¥ Redis ç¼“å­˜ä½œä¸šç»Ÿè®¡ä¿¡æ¯ï¼Œttl=10m");

            return statistics;
        } catch (Exception e) {
            log.error("è·å–ä½œä¸šç»Ÿè®¡å¤±è´¥", e);
            Map<String, Object> errorStats = new HashMap<>();
            errorStats.put("error", "ç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥");
            return errorStats;
        }
    }

    /**
     * è·å–ä½œä¸šæ‰§è¡Œè¶‹åŠ¿
     */
    public List<Map<String, Object>> getJobExecutionTrend(int days) {
        try {
            LocalDateTime startTime = LocalDateTime.now().minusDays(days);
            List<Map<String, Object>> trendData = sparkJobLogsMapper.getJobExecutionTrend(startTime);
            return trendData;
        } catch (Exception e) {
            log.error("è·å–ä½œä¸šæ‰§è¡Œè¶‹åŠ¿å¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * æ ¹æ®IDè·å–ä½œä¸šè¯¦æƒ…
     */
    public SparkJobLogsDTO getJobById(Long id) {
        try {
            SparkJobLogs job = sparkJobLogsMapper.findById(id);
            return job != null ? convertToDTO(job) : null;
        } catch (Exception e) {
            log.error("æ ¹æ®IDè·å–ä½œä¸šå¤±è´¥", e);
            return null;
        }
    }

    /**
     * æ¸…ç†æ—§çš„ä½œä¸šæ—¥å¿—
     */
    public int cleanupOldJobs(int daysToKeep) {
        try {
            LocalDateTime cutoffTime = LocalDateTime.now().minusDays(daysToKeep);
            int deletedCount = sparkJobLogsMapper.cleanupOldJobs(cutoffTime);
            if (deletedCount > 0) {
                // æ¸…é™¤ç›¸å…³ç¼“å­˜
                clearJobsCache();
                log.info("ğŸ—‘ï¸ æ¸…ç†æ—§ä½œä¸šæ—¥å¿—å®Œæˆï¼Œåˆ é™¤ {} æ¡è®°å½•", deletedCount);
            }
            return deletedCount;
        } catch (Exception e) {
            log.error("æ¸…ç†æ—§ä½œä¸šæ—¥å¿—å¤±è´¥", e);
            return 0;
        }
    }

    /**
     * æ¸…é™¤ä½œä¸šç›¸å…³ç¼“å­˜
     */
    private void clearJobsCache() {
        try {
            redisTemplate.delete(redisTemplate.keys("spark_jobs:*"));
            log.info("ğŸ—‘ï¸ å·²æ¸…é™¤ä½œä¸šç›¸å…³ç¼“å­˜");
        } catch (Exception e) {
            log.warn("æ¸…é™¤ä½œä¸šç¼“å­˜å¤±è´¥", e);
        }
    }

    private SparkJobLogsDTO convertToDTO(SparkJobLogs logs) {
        SparkJobLogsDTO dto = new SparkJobLogsDTO();
        dto.setId(logs.getId());
        dto.setJobName(logs.getJobName());
        dto.setStartTime(logs.getStartTime());
        dto.setEndTime(logs.getEndTime());
        dto.setStatus(logs.getStatus());
        dto.setInputDeliverPath(logs.getInputDeliverPath());
        dto.setInputPickupPath(logs.getInputPickupPath());
        dto.setOutputPath(logs.getOutputPath());
        dto.setProcessedRecords(logs.getProcessedRecords());
        dto.setErrorMessage(logs.getErrorMessage());
        dto.setExecutionTimeSeconds(logs.getExecutionTimeSeconds());
        dto.setTimeFormatUsed(logs.getTimeFormatUsed());
        dto.setDefaultYear(logs.getDefaultYear());
        dto.setCreatedAt(logs.getCreatedAt());
        return dto;
    }
}