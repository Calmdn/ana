package com.logistics.service.service;

import com.logistics.service.dao.entity.SparkJobLogs;
import com.logistics.service.dao.mapper.SparkJobLogsMapper;
import com.logistics.service.dto.SparkJobLogsDTO;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.util.concurrent.TimeUnit;
import java.util.stream.Collectors;
import java.time.LocalDateTime;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.HashMap;

@Slf4j
@Service
@Transactional(readOnly = true)
public class SparkJobLogsService {

    @Autowired
    private SparkJobLogsMapper sparkJobLogsMapper;

    @Autowired
    private RedisTemplate<String, Object> redisTemplate;

    /**
     * ä¿å­˜ä½œä¸šæ—¥å¿—
     */
    @Transactional
    public int saveJob(SparkJobLogs job) {
        validateJob(job);
        return sparkJobLogsMapper.insertJob(job);
    }

    /**
     * æ‰¹é‡ä¿å­˜ä½œä¸šæ—¥å¿—
     */
    @Transactional
    public int batchSaveJobs(List<SparkJobLogs> jobList) {
        for (SparkJobLogs job : jobList) {
            validateJob(job);
        }
        return sparkJobLogsMapper.batchInsertJobs(jobList);
    }

    /**
     * è·å–æœ€è¿‘çš„ä½œä¸šæ—¥å¿—
     */
    public List<SparkJobLogsDTO> getRecentJobLogs(int limit) {
        try {
            String key = "spark_jobs:recent:" + limit;

            @SuppressWarnings("unchecked")
            List<SparkJobLogsDTO> cache = (List<SparkJobLogsDTO>) redisTemplate.opsForValue().get(key);
            if (cache != null) {
                log.info("âœ… ä» Redis ç¼“å­˜è·å–æœ€è¿‘ä½œä¸šæ—¥å¿—, size={}", cache.size());
                return cache;
            }

            log.info("ğŸ” Redis æœªå‘½ä¸­ï¼ŒæŸ¥è¯¢ MySQL æœ€è¿‘ä½œä¸šæ—¥å¿—");
            List<SparkJobLogs> logs = sparkJobLogsMapper.findRecentJobs(limit);

            List<SparkJobLogsDTO> result = logs.stream()
                    .map(this::convertToDTO)
                    .collect(Collectors.toList());

            if (!result.isEmpty()) {
                redisTemplate.opsForValue().set(key, result, 5, TimeUnit.MINUTES);
                log.info("ğŸ’¾ å†™å…¥ Redis ç¼“å­˜æœ€è¿‘ä½œä¸šæ—¥å¿—ï¼Œttl=5m");
            }

            return result;
        } catch (Exception e) {
            log.error("è·å–æœ€è¿‘ä½œä¸šæ—¥å¿—å¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * æŒ‰çŠ¶æ€è·å–ä½œä¸šæ—¥å¿—
     */
    public List<SparkJobLogsDTO> getJobLogsByStatus(String status) {
        try {
            String key = "spark_jobs:status:" + status;

            @SuppressWarnings("unchecked")
            List<SparkJobLogsDTO> cache = (List<SparkJobLogsDTO>) redisTemplate.opsForValue().get(key);
            if (cache != null) {
                log.info("âœ… ä» Redis ç¼“å­˜è·å–çŠ¶æ€ä½œä¸šæ—¥å¿—[status={}], size={}", status, cache.size());
                return cache;
            }

            List<SparkJobLogs> logs = sparkJobLogsMapper.findByStatus(status);
            List<SparkJobLogsDTO> result = logs.stream()
                    .map(this::convertToDTO)
                    .collect(Collectors.toList());

            if (!result.isEmpty()) {
                redisTemplate.opsForValue().set(key, result, 3, TimeUnit.MINUTES);
                log.info("ğŸ’¾ å†™å…¥ Redis ç¼“å­˜çŠ¶æ€ä½œä¸šæ—¥å¿—[status={}]ï¼Œttl=3m", status);
            }

            return result;
        } catch (Exception e) {
            log.error("æŒ‰çŠ¶æ€è·å–ä½œä¸šæ—¥å¿—å¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * æŒ‰ä½œä¸šåç§°è·å–æ—¥å¿—
     */
    public List<SparkJobLogsDTO> getJobLogsByName(String jobName) {
        try {
            List<SparkJobLogs> logs = sparkJobLogsMapper.findByJobName(jobName);
            return logs.stream()
                    .map(this::convertToDTO)
                    .collect(Collectors.toList());
        } catch (Exception e) {
            log.error("æŒ‰ä½œä¸šåç§°è·å–æ—¥å¿—å¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * è·å–å¤±è´¥çš„ä½œä¸š
     */
    public List<SparkJobLogsDTO> getFailedJobs() {
        return getJobLogsByStatus("FAILED");
    }

    /**
     * è·å–è¿è¡Œä¸­çš„ä½œä¸š
     */
    public List<SparkJobLogsDTO> getRunningJobs() {
        return getJobLogsByStatus("RUNNING");
    }

    /**
     * è·å–æˆåŠŸçš„ä½œä¸š
     */
    public List<SparkJobLogsDTO> getSuccessfulJobs(int limit) {
        try {
            List<SparkJobLogs> logs = sparkJobLogsMapper.findSuccessfulJobs(limit);
            return logs.stream()
                    .map(this::convertToDTO)
                    .collect(Collectors.toList());
        } catch (Exception e) {
            log.error("è·å–æˆåŠŸä½œä¸šå¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * è·å–æŒ‡å®šæ—¶é—´èŒƒå›´çš„ä½œä¸šæ—¥å¿—
     */
    public List<SparkJobLogsDTO> getJobLogsByTimeRange(LocalDateTime startTime, LocalDateTime endTime) {
        try {
            String key = String.format("spark_jobs:range:%s:%s", startTime, endTime);

            @SuppressWarnings("unchecked")
            List<SparkJobLogsDTO> cache = (List<SparkJobLogsDTO>) redisTemplate.opsForValue().get(key);
            if (cache != null) {
                log.info("âœ… ä» Redis ç¼“å­˜è·å–æ—¶é—´èŒƒå›´ä½œä¸šæ—¥å¿—, size={}", cache.size());
                return cache;
            }

            List<SparkJobLogs> logs = sparkJobLogsMapper.findByTimeRange(startTime, endTime);
            List<SparkJobLogsDTO> result = logs.stream()
                    .map(this::convertToDTO)
                    .collect(Collectors.toList());

            if (!result.isEmpty()) {
                redisTemplate.opsForValue().set(key, result, 15, TimeUnit.MINUTES);
                log.info("ğŸ’¾ å†™å…¥ Redis ç¼“å­˜æ—¶é—´èŒƒå›´ä½œä¸šæ—¥å¿—ï¼Œttl=15m");
            }

            return result;
        } catch (Exception e) {
            log.error("æŒ‰æ—¶é—´èŒƒå›´è·å–ä½œä¸šæ—¥å¿—å¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * è·å–ä»Šæ—¥ä½œä¸šæ—¥å¿—
     */
    public List<SparkJobLogsDTO> getTodayJobs() {
        LocalDateTime startOfDay = LocalDateTime.now().withHour(0).withMinute(0).withSecond(0).withNano(0);
        LocalDateTime endOfDay = startOfDay.plusDays(1).minusNanos(1);
        return getJobLogsByTimeRange(startOfDay, endOfDay);
    }

    /**
     * è·å–é•¿æ—¶é—´è¿è¡Œçš„ä½œä¸š
     */
    public List<SparkJobLogsDTO> getLongRunningJobs(int thresholdMinutes) {
        try {
            List<SparkJobLogs> logs = sparkJobLogsMapper.findLongRunningJobs(thresholdMinutes * 60);
            return logs.stream()
                    .map(this::convertToDTO)
                    .collect(Collectors.toList());
        } catch (Exception e) {
            log.error("è·å–é•¿æ—¶é—´è¿è¡Œä½œä¸šå¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * è·å–æœ‰é”™è¯¯çš„ä½œä¸š
     */
    public List<SparkJobLogsDTO> getJobsWithErrors(int limit) {
        try {
            List<SparkJobLogs> logs = sparkJobLogsMapper.findJobsWithErrors(limit);
            return logs.stream()
                    .map(this::convertToDTO)
                    .collect(Collectors.toList());
        } catch (Exception e) {
            log.error("è·å–æœ‰é”™è¯¯çš„ä½œä¸šå¤±è´¥", e);
            return new ArrayList<>();
        }
    }

    /**
     * è·å–ä½œä¸šç»Ÿè®¡ä¿¡æ¯
     */
    public Map<String, Object> getJobStatistics() {
        try {
            String key = "spark_jobs:statistics";

            @SuppressWarnings("unchecked")
            Map<String, Object> cache = (Map<String, Object>) redisTemplate.opsForValue().get(key);
            if (cache != null) {
                log.info("âœ… ä» Redis ç¼“å­˜è·å–ä½œä¸šç»Ÿè®¡ä¿¡æ¯");
                return cache;
            }

            Map<String, Object> statistics = new HashMap<>();

            // è·å–æ€»æ•°é‡
            int totalJobs = sparkJobLogsMapper.countTotalJobs();
            int successJobs = sparkJobLogsMapper.countJobsByStatus("SUCCESS");
            int failedJobs = sparkJobLogsMapper.countJobsByStatus("FAILED");
            int runningJobs = sparkJobLogsMapper.countJobsByStatus("RUNNING");

            // è®¡ç®—æ¯”ç‡
            double successRate = totalJobs > 0 ? (double) successJobs / totalJobs * 100 : 0;
            double failureRate = totalJobs > 0 ? (double) failedJobs / totalJobs * 100 : 0;

            // è·å–å¹³å‡æ‰§è¡Œæ—¶é—´å’Œæ€»å¤„ç†è®°å½•æ•°
            Double avgExecutionTime = sparkJobLogsMapper.getAverageExecutionTime();
            Long totalProcessedRecords = sparkJobLogsMapper.getTotalProcessedRecords();

            statistics.put("totalJobs", totalJobs);
            statistics.put("successJobs", successJobs);
            statistics.put("failedJobs", failedJobs);
            statistics.put("runningJobs", runningJobs);
            statistics.put("successRate", Math.round(successRate * 100.0) / 100.0);
            statistics.put("failureRate", Math.round(failureRate * 100.0) / 100.0);
            statistics.put("avgExecutionTime", avgExecutionTime != null ? Math.round(avgExecutionTime) : 0);
            statistics.put("totalProcessedRecords", totalProcessedRecords != null ? totalProcessedRecords : 0L);

            redisTemplate.opsForValue().set(key, statistics, 10, TimeUnit.MINUTES);
            log.info("ğŸ’¾ å†™å…¥ Redis ç¼“å­˜ä½œä¸šç»Ÿè®¡ä¿¡æ¯ï¼Œttl=10m");

            return statistics;
        } catch (Exception e) {
            log.error("è·å–ä½œä¸šç»Ÿè®¡å¤±è´¥", e);
            Map<String, Object> errorStats = new HashMap<>();
            errorStats.put("error", "ç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥");
            return errorStats;
        }
    }

    /**
     * è·å–ä½œä¸šæ‰§è¡Œè¶‹åŠ¿
     */
    public List<Map<String, Object>> getJobExecutionTrend(int days) {
        try {
            LocalDateTime startTime = LocalDateTime.now().minusDays(days);
            return sparkJobLogsMapper.getJobExecutionTrend(startTime);
        } catch (Exception e) {
            log.error("è·å–ä½œä¸šæ‰§è¡Œè¶‹åŠ¿å¤±è´¥", e);
            return new ArrayList<>();
        }
    }


    /**
     * æ ¹æ®IDè·å–ä½œä¸šè¯¦æƒ…
     */
    public SparkJobLogsDTO getJobById(Long id) {
        try {
            SparkJobLogs job = sparkJobLogsMapper.findById(id);
            return job != null ? convertToDTO(job) : null;
        } catch (Exception e) {
            log.error("æ ¹æ®IDè·å–ä½œä¸šå¤±è´¥", e);
            return null;
        }
    }

    /**
     * æ›´æ–°ä½œä¸šçŠ¶æ€
     */
    @Transactional
    public int updateJobStatus(Long id, String status, LocalDateTime endTime, Integer executionTimeSeconds, String errorMessage) {
        return sparkJobLogsMapper.updateJobStatus(id, status, endTime, executionTimeSeconds, errorMessage);
    }

    /**
     * æ¸…ç†æ—§çš„ä½œä¸šæ—¥å¿—
     */
    @Transactional
    public int cleanupOldJobs(int daysToKeep) {
        try {
            LocalDateTime cutoffTime = LocalDateTime.now().minusDays(daysToKeep);
            int deletedCount = sparkJobLogsMapper.cleanupOldJobs(cutoffTime);
            if (deletedCount > 0) {
                // æ¸…é™¤ç›¸å…³ç¼“å­˜
                clearJobsCache();
                log.info("ğŸ—‘ï¸ æ¸…ç†æ—§ä½œä¸šæ—¥å¿—å®Œæˆï¼Œåˆ é™¤ {} æ¡è®°å½•", deletedCount);
            }
            return deletedCount;
        } catch (Exception e) {
            log.error("æ¸…ç†æ—§ä½œä¸šæ—¥å¿—å¤±è´¥", e);
            return 0;
        }
    }

    /**
     * æ•°æ®éªŒè¯
     */
    private void validateJob(SparkJobLogs job) {
        if (job.getJobName() == null || job.getJobName().trim().isEmpty()) {
            throw new IllegalArgumentException("ä½œä¸šåç§°ä¸èƒ½ä¸ºç©º");
        }
        if (job.getStatus() == null || job.getStatus().trim().isEmpty()) {
            throw new IllegalArgumentException("ä½œä¸šçŠ¶æ€ä¸èƒ½ä¸ºç©º");
        }
    }

    /**
     * æ¸…é™¤ä½œä¸šç›¸å…³ç¼“å­˜
     */
    private void clearJobsCache() {
        try {
            redisTemplate.delete(redisTemplate.keys("spark_jobs:*"));
            log.info("ğŸ—‘ï¸ å·²æ¸…é™¤ä½œä¸šç›¸å…³ç¼“å­˜");
        } catch (Exception e) {
            log.warn("æ¸…é™¤ä½œä¸šç¼“å­˜å¤±è´¥", e);
        }
    }

    /**
     * å®ä½“è½¬DTO
     */
    private SparkJobLogsDTO convertToDTO(SparkJobLogs logs) {
        SparkJobLogsDTO dto = new SparkJobLogsDTO();
        dto.setId(logs.getId());
        dto.setJobName(logs.getJobName());
        dto.setStartTime(logs.getStartTime());
        dto.setEndTime(logs.getEndTime());
        dto.setStatus(logs.getStatus());
        dto.setInputDeliverPath(logs.getInputDeliverPath());
        dto.setInputPickupPath(logs.getInputPickupPath());
        dto.setOutputPath(logs.getOutputPath());
        dto.setProcessedRecords(logs.getProcessedRecords());
        dto.setErrorMessage(logs.getErrorMessage());
        dto.setExecutionTimeSeconds(logs.getExecutionTimeSeconds());
        dto.setTimeFormatUsed(logs.getTimeFormatUsed());
        dto.setDefaultYear(logs.getDefaultYear());
        dto.setCreatedAt(logs.getCreatedAt());
        return dto;
    }
}