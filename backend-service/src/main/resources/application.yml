# 服务配置
server:
  port: 8080
#  servlet:
#    context-path: /logistics-api

spring:
  application:
    name: logistics-analysis-service

  # 数据源配置
  datasource:
    type: com.alibaba.druid.pool.DruidDataSource
    driver-class-name: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://localhost:3306/logistics_db?useSSL=false&serverTimezone=UTC&allowPublicKeyRetrieval=true
    username: root
    password: root

    # Druid连接池配置
    druid:
      initial-size: 5
      min-idle: 5
      max-active: 20
      max-wait: 60000
      validation-query: SELECT 1
      test-while-idle: true

  # Redis配置
  redis:
    host: localhost
    port: 6379
    database: 0
    timeout: 5000ms
    jedis:
      pool:
        max-active: 20
        max-idle: 10
        min-idle: 5
        max-wait: 1000ms

# MyBatis配置
mybatis:
  mapper-locations: classpath:mapper/*.xml
  type-aliases-package: com.logistics.service.dto
  configuration:
    map-underscore-to-camel-case: true

# 物流分析平台配置
logistics:
  spark:
    home: /usr/local/spark
    app-jar: ./spark-analysis/target/spark-analysis-1.0.0.jar
    master: local[*]

  hdfs:
    base-url: hdfs://localhost:9000
    input:
      deliver-path: /user/calmdn/lade/raw/deliver/*.csv
      pickup-path: /user/calmdn/lade/raw/pickup/*.csv
    output:
      base-path: /user/calmdn/lade/results/enhanced-analysis

# Redis监控配置
redis:
  monitoring:
    enabled: true  # 启用Redis监控
    log-dir: "logs/redis"  # 目录

    # 监控范围
    included-packages:
      - "com.logistics.service"

    excluded-packages:
      - "com.logistics.test"
      - "com.logistics.config"

    # 缓存监控配置
    included-caches:
      - "time_efficiency"  # 您TimeEfficiencyService中使用的缓存
      - "stats"
      - "spatial"
      - "jobs"
      - "predictions"
      - "efficiency"
      - "kpi"
      - "costs"
      - "reports"
      - "alerts"

    excluded-caches: []

    # 性能过滤配置
    min-duration-ms: 2  # 只记录超过2ms的操作
    log-cache-hits: true  # 记录缓存命中
    log-cache-misses: true  # 记录缓存未命中

    # 日志文件管理
    max-log-files: 30  # 保留30天的日志文件

    # 监控详细程度
    detail-level: "STANDARD"  # BASIC, STANDARD, DETAILED


# 日志配置
logging:
  level:
    com.logistics.service: INFO
    org.apache.spark: WARN
    com.logistics.monitoring: INFO
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"